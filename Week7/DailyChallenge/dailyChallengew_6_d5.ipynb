{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMaRs1Ez0dv/iCE2p37Xwp4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amoukrim/AI/blob/main/Week7/DailyChallenge/dailyChallengew_6_d5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "@Author : ADil MOUKRIM\n",
        "#Text summarization using NLP\n",
        "Last Updated: April 8th, 2025\n",
        "\n",
        "Daily Challenge : Text summarization using NLP\n",
        "\n",
        "\n",
        "Introduction\n",
        "This notebook demonstrates a practical application of Natural Language Processing (NLP) techniques to automatically generate summaries of text documents. We will explore how to preprocess text data, represent words and sentences as vectors, and leverage graph-based algorithms to identify the most important sentences for summarization.\n",
        "\n",
        "\n",
        "\n",
        "üë©‚Äçüè´ üë©üèø‚Äçüè´ What You‚Äôll learn\n",
        "Text Preprocessing: Techniques for cleaning and preparing text data, including tokenization, stop word removal, and converting text to lowercase.\n",
        "Word Embeddings: Understanding and using pre-trained word embeddings like GloVe to represent words as dense vectors.\n",
        "Sentence Vectorization: Creating vector representations of sentences by aggregating word embeddings.\n",
        "Similarity Measures: Using cosine similarity to determine the semantic similarity between sentences.\n",
        "Graph-Based Summarization: Applying the PageRank algorithm on a graph of sentence similarities to rank sentence importance.\n",
        "Text Summarization Implementation: Combining these techniques to build a text summarization system.\n",
        "\n",
        "\n",
        "üõ†Ô∏è What you will create\n",
        "You will create an automatic text summarization system that can take a collection of tennis articles as input and generate a concise summary highlighting the key information.\n",
        "\n",
        "\n",
        "\n",
        "Task\n",
        "1. Data Loading and Inspection\n",
        "\n",
        "Load the tennis articles dataset from the .xls file using pandas.\n",
        "Explore the dataset using .head() and .info() to understand its structure.\n",
        "Drop the article_title column to simplify the dataset.\n",
        "2. Sentence Tokenization\n",
        "\n",
        "Use nltk.sent_tokenize() to split the article_text into individual sentences.\n",
        "Flatten the resulting list of sentence lists into a single list of all sentences.\n",
        "3. Download and Load GloVe Word Embeddings\n",
        "\n",
        "Download the pre-trained GloVe vectors (e.g., glove.6B.100d.txt).\n",
        "Load the embeddings into a Python dictionary where each word maps to its 100-dimensional vector.\n",
        "4. Text Cleaning and Normalization\n",
        "\n",
        "Remove punctuation, special characters, and numbers using regex.\n",
        "Convert all sentences to lowercase to avoid case-sensitive mismatch.\n",
        "Remove stop words using nltk.corpus.stopwords to reduce noise in the data.\n",
        "5. Sentence Vectorization\n",
        "\n",
        "For each cleaned sentence:\n",
        "Split into words.\n",
        "Replace each word with its GloVe vector (use a zero-vector if the word is not in the embedding).\n",
        "Compute the average of all word vectors in the sentence.\n",
        "Store all resulting sentence vectors in a list.\n",
        "6. Similarity Matrix Construction\n",
        "\n",
        "Initialize an empty matrix of size (number of sentences √ó number of sentences).\n",
        "Compute pairwise cosine similarity between sentence vectors.\n",
        "Fill in the matrix such that each cell represents the similarity between two sentences.\n",
        "7. Graph Construction and Sentence Ranking\n",
        "\n",
        "Convert the similarity matrix into a graph using networkx.\n",
        "Apply the PageRank algorithm to score the importance of each sentence.\n",
        "8. Summarization\n",
        "\n",
        "Sort all sentences based on their PageRank scores in descending order.\n",
        "Extract the top N sentences (e.g., 10) as the final summary.\n",
        "Print or return the summarized sentences."
      ],
      "metadata": {
        "id": "9OZeqO8DbkDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#√âtape 1 : Chargement et inspection des donn√©es"
      ],
      "metadata": {
        "id": "iHTFDcxBbfsx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZ7z70T4bbvA",
        "outputId": "b645cc53-6c4d-4981-bd36-e69c46c83517"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Fichier charg√© avec succ√®s (encoding='latin-1')\n",
            "\n",
            "=== APERCU DES PREMI√àRES LIGNES ===\n",
            "   article_id                                      article_title  \\\n",
            "0           1  I do not have friends in¬†tennis, says Maria Sh...   \n",
            "1           2  Federer defeats Medvedev to advance to 14th Sw...   \n",
            "2           3  Tennis: Roger Federer ignored deadline set by ...   \n",
            "3           4  Nishikori to face off against Anderson in Vien...   \n",
            "4           5  Roger Federer has made this huge change to ten...   \n",
            "\n",
            "                                        article_text  \\\n",
            "0  Maria Sharapova has basically no friends as te...   \n",
            "1  BASEL, Switzerland (AP) ¬ó Roger Federer advanc...   \n",
            "2  Roger Federer has revealed that organisers of ...   \n",
            "3  Kei Nishikori will try to end his long losing ...   \n",
            "4  Federer, 37, first broke through on tour over ...   \n",
            "\n",
            "                                              source  \n",
            "0  https://www.tennisworldusa.org/tennis/news/Mar...  \n",
            "1  http://www.tennis.com/pro-game/2018/10/copil-s...  \n",
            "2  https://scroll.in/field/899938/tennis-roger-fe...  \n",
            "3  http://www.tennis.com/pro-game/2018/10/nishiko...  \n",
            "4  https://www.express.co.uk/sport/tennis/1036101...  \n",
            "\n",
            "=== INFORMATIONS SUR LE DATASET ===\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8 entries, 0 to 7\n",
            "Data columns (total 4 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   article_id     8 non-null      int64 \n",
            " 1   article_title  8 non-null      object\n",
            " 2   article_text   8 non-null      object\n",
            " 3   source         8 non-null      object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 388.0+ bytes\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Import des biblioth√®ques n√©cessaires\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# √âtape 1 : Chargement du dataset avec encodage appropri√©\n",
        "# Tentative avec Latin-1 qui couvre la plupart des caract√®res europ√©ens\n",
        "try:\n",
        "    df = pd.read_csv('/content/tennis_articles.csv', encoding='latin-1')\n",
        "    print(\"‚úÖ Fichier charg√© avec succ√®s (encoding='latin-1')\")\n",
        "except Exception as e:\n",
        "    # Si Latin-1 √©choue, tentative avec Windows-1252\n",
        "    print(f\"‚ö†Ô∏è Erreur avec latin-1 : {str(e)[:50]}...\")\n",
        "    try:\n",
        "        df = pd.read_csv('/content/tennis_articles.csv', encoding='cp1252')\n",
        "        print(\"‚úÖ Fichier charg√© avec succ√®s (encoding='cp1252')\")\n",
        "    except Exception as e2:\n",
        "        # Derni√®re tentative avec UTF-8 et gestion des erreurs\n",
        "        print(f\"‚ö†Ô∏è Erreur avec cp1252 : {str(e2)[:50]}...\")\n",
        "        df = pd.read_csv('/content/tennis_articles.csv', encoding='utf-8', errors='ignore')\n",
        "        print(\"‚úÖ Fichier charg√© avec gestion des erreurs d'encodage\")\n",
        "\n",
        "# Exploration initiale du dataset\n",
        "print(\"\\n=== APERCU DES PREMI√àRES LIGNES ===\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\n=== INFORMATIONS SUR LE DATASET ===\")\n",
        "print(df.info())\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simplification du dataset\n",
        "\n",
        "df = df.drop(columns=['article_title'])\n",
        "print(\"\\n‚úÖ Colonne 'article_title' supprim√©e\")\n",
        "print(f\"\\nüìä Le dataset contient maintenant {len(df)} articles\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0W3JksLjdyAy",
        "outputId": "0b325aa6-806b-4022-e639-108d44e00914"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Colonne 'article_title' supprim√©e\n",
            "\n",
            "üìä Le dataset contient maintenant 8 articles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#√âtape 2 : Tokenisation des phrases\n",
        "L'Objectif est de d√©couper tous les articles en phrases individuelles pour pr√©parer l'analyse."
      ],
      "metadata": {
        "id": "Y9Nok5ZGeUFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cette solution est une alternative √† la solustion NLTK qui se charge pas"
      ],
      "metadata": {
        "id": "eiFt4YzBfytr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# M√©thode 100% Python sans NLTK (fallback)\n",
        "import re\n",
        "\n",
        "def simple_sentence_split(text):\n",
        "    \"\"\"D√©coupage simple des phrases en utilisant la ponctuation\"\"\"\n",
        "    # Expression r√©guli√®re pour identifier les fins de phrases\n",
        "    sentence_endings = re.compile(r'(?<=[.!?])\\s+')\n",
        "    sentences = sentence_endings.split(text)\n",
        "    return [s.strip() for s in sentences if s.strip() and len(s) > 10]\n",
        "\n",
        "# Utilisation de la m√©thode alternative\n",
        "all_sentences = []\n",
        "for article in df['article_text']:\n",
        "    sentences = simple_sentence_split(article)\n",
        "    all_sentences.extend(sentences)\n",
        "\n",
        "print(\"‚úÖ Tokenisation alternative r√©ussie\")\n",
        "print(f\"Total de phrases: {len(all_sentences)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUNKZKiEeeGp",
        "outputId": "51dc536c-99c1-4c5c-fe23-b415f3c265dd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Tokenisation alternative r√©ussie\n",
            "Total de phrases: 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# √âtape 3 : T√©l√©chargement et chargement des embeddings GloVe\n",
        "L'Objectif est d'obtenir des vecteurs de mots pr√©-entra√Æn√©s (100 dimensions) pour repr√©senter s√©mantiquement chaque mot."
      ],
      "metadata": {
        "id": "vCzWWThlgU9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "# √âtape 3 : T√©l√©chargement des embeddings GloVe\n",
        "# Les embeddings GloVe sont des vecteurs de mots pr√©-entra√Æn√©s sur des corpus massifs\n",
        "\n",
        "print(\"üì• T√©l√©chargement des embeddings GloVe...\")\n",
        "\n",
        "# URL officielle de Stanford pour GloVe 6B (100d)\n",
        "glove_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "glove_zip_path = \"/content/glove.6B.zip\"\n",
        "glove_extract_path = \"/content/glove_embeddings\"\n",
        "\n",
        "# T√©l√©chargement si non d√©j√† pr√©sent\n",
        "if not os.path.exists(glove_extract_path):\n",
        "    # Cr√©ation du dossier\n",
        "    os.makedirs(glove_extract_path, exist_ok=True)\n",
        "\n",
        "    # T√©l√©chargement du fichier zip (environ 862MB)\n",
        "    print(\"‚è≥ T√©l√©chargement du fichier GloVe (peut prendre 1-2 minutes)...\")\n",
        "    urllib.request.urlretrieve(glove_url, glove_zip_path)\n",
        "\n",
        "    # Extraction des fichiers\n",
        "    print(\"üì¶ Extraction des fichiers...\")\n",
        "    with zipfile.ZipFile(glove_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(glove_extract_path)\n",
        "\n",
        "    # Nettoyage : suppression du zip\n",
        "    os.remove(glove_zip_path)\n",
        "    print(\"‚úÖ T√©l√©chargement et extraction termin√©s!\")\n",
        "\n",
        "# Chargement des embeddings dans un dictionnaire\n",
        "# Nous utilisons glove.6B.100d.txt pour avoir 100 dimensions par mot\n",
        "glove_file = os.path.join(glove_extract_path, \"glove.6B.100d.txt\")\n",
        "word_embeddings = {}\n",
        "\n",
        "print(\" Chargement des embeddings dans la m√©moire...\")\n",
        "with open(glove_file, encoding='utf-8') as f:\n",
        "    # Chaque ligne contient : mot + 100 valeurs num√©riques\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]  # Premier √©l√©ment = le mot\n",
        "        vector = np.asarray(values[1:], dtype='float32')  # 100 dimensions\n",
        "        word_embeddings[word] = vector\n",
        "\n",
        "# V√©rification du chargement\n",
        "print(f\" Chargement termin√©! {len(word_embeddings)} mots charg√©s\")\n",
        "print(f\" Dimensions des vecteurs: {len(next(iter(word_embeddings.values())))}\")\n",
        "\n",
        "# Test rapide avec quelques mots de tennis\n",
        "test_words = ['tennis', 'federer', 'sharapova', 'match']\n",
        "print(\"\\n=== TEST DES EMBEDDINGS ===\")\n",
        "for word in test_words:\n",
        "    if word in word_embeddings:\n",
        "        print(f\"‚úÖ '{word}' trouv√© - Extrait du vecteur: {word_embeddings[word][:5]}...\")\n",
        "    else:\n",
        "        print(f\"‚ùå '{word}' non trouv√© dans les embeddings\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VILpdZPhgeqx",
        "outputId": "7483ef57-f3e7-4142-f4a1-7cdf43570990"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• T√©l√©chargement des embeddings GloVe...\n",
            "‚è≥ T√©l√©chargement du fichier GloVe (peut prendre 1-2 minutes)...\n",
            "üì¶ Extraction des fichiers...\n",
            "‚úÖ T√©l√©chargement et extraction termin√©s!\n",
            "üìö Chargement des embeddings dans la m√©moire...\n",
            "‚úÖ Chargement termin√©! 400000 mots charg√©s\n",
            "üìè Dimensions des vecteurs: 100\n",
            "\n",
            "=== TEST DES EMBEDDINGS ===\n",
            "‚úÖ 'tennis' trouv√© - Extrait du vecteur: [ 0.21508  0.61981  0.84039  0.71394 -0.29904]...\n",
            "‚úÖ 'federer' trouv√© - Extrait du vecteur: [ 0.22673  -0.048534  0.64561   0.69949   0.57822 ]...\n",
            "‚úÖ 'sharapova' trouv√© - Extrait du vecteur: [0.12194 0.26347 1.2314  0.90343 0.03207]...\n",
            "‚úÖ 'match' trouv√© - Extrait du vecteur: [-0.27317   0.024643  0.60197   0.10075  -0.91521 ]...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#√âtape 4 : Nettoyage et normalisation du texte\n",
        "L'Objectif est de pr√©parer les phrases pour la vectorisation en √©liminant le bruit (ponctuation, stop words, etc.)."
      ],
      "metadata": {
        "id": "7XWEguxMiPrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import des biblioth√®ques n√©cessaires\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# T√©l√©chargement des stopwords\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "#  Nettoyage et normalisation du texte\n",
        "print(\" D√©but du nettoyage et de la normalisation...\")\n",
        "\n",
        "# R√©cup√©ration de la liste des stopwords anglais\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(f\"üìã {len(stop_words)} stopwords charg√©s (ex: {list(stop_words)[:10]})\")\n",
        "\n",
        "# Fonction de nettoyage compl√®te\n",
        "def clean_sentence(sentence):\n",
        "    \"\"\"\n",
        "    Nettoie une phrase en appliquant plusieurs √©tapes :\n",
        "    1. Conversion en minuscules\n",
        "    2. Suppression des caract√®res sp√©ciaux et chiffres\n",
        "    3. Suppression des stopwords\n",
        "    4. Suppression des espaces multiples\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Conversion en minuscules\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    # 2. Suppression des caract√®res sp√©ciaux et chiffres\n",
        "    # Garde uniquement les lettres et les espaces\n",
        "    sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n",
        "\n",
        "    # 3. Tokenisation en mots\n",
        "    words = sentence.split()\n",
        "\n",
        "    # 4. Suppression des stopwords et mots trop courts (< 2 lettres)\n",
        "    cleaned_words = [word for word in words\n",
        "                    if word not in stop_words and len(word) > 2]\n",
        "\n",
        "    # 5. Reconstruction de la phrase\n",
        "    cleaned_sentence = ' '.join(cleaned_words)\n",
        "\n",
        "    return cleaned_sentence\n",
        "\n",
        "# Application du nettoyage √† toutes les phrases\n",
        "print(\"\\n Nettoyage des phrases en cours...\")\n",
        "cleaned_sentences = []\n",
        "\n",
        "for idx, sentence in enumerate(all_sentences):\n",
        "    cleaned = clean_sentence(sentence)\n",
        "    if cleaned:  # Garde uniquement les phrases non vides\n",
        "        cleaned_sentences.append(cleaned)\n",
        "\n",
        "    # Affichage du progr√®s pour les premi√®res phrases\n",
        "    if idx < 3:\n",
        "        print(f\"\\n Phrase originale {idx+1}:\")\n",
        "        print(f\"   {sentence[:100]}...\")\n",
        "        print(f\" Apr√®s nettoyage:\")\n",
        "        print(f\"   {cleaned}\")\n",
        "\n",
        "# Statistiques post-nettoyage\n",
        "print(\"\\n=== R√âSUM√â DU NETTOYAGE ===\")\n",
        "print(f\" Phrases avant nettoyage: {len(all_sentences)}\")\n",
        "print(f\" Phrases apr√®s nettoyage: {len(cleaned_sentences)}\")\n",
        "print(f\" Phrases √©limin√©es: {len(all_sentences) - len(cleaned_sentences)}\")\n",
        "\n",
        "# Aper√ßu des premi√®res phrases nettoy√©es\n",
        "print(\"\\n=== APERCU DES PHRASES NETTOY√âES ===\")\n",
        "for i, sentence in enumerate(cleaned_sentences[:5]):\n",
        "    print(f\"{i+1}. {sentence}\")\n",
        "\n",
        "# V√©rification du vocabulaire couvert par GloVe\n",
        "print(\"\\n=== V√âRIFICATION DU VOCABULAIRE ===\")\n",
        "all_words = ' '.join(cleaned_sentences).split()\n",
        "unique_words = set(all_words)\n",
        "found_words = [word for word in unique_words if word in word_embeddings]\n",
        "coverage = len(found_words) / len(unique_words) * 100\n",
        "\n",
        "print(f\" Mots uniques trouv√©s: {len(found_words)}/{len(unique_words)}\")\n",
        "print(f\"Couverture GloVe: {coverage:.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSOQ5FCGicu6",
        "outputId": "0b5162be-ae72-4362-c8a1-0dc1bb21b369"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " D√©but du nettoyage et de la normalisation...\n",
            "üìã 198 stopwords charg√©s (ex: [\"you've\", 'isn', 'from', \"doesn't\", 'into', 'after', 'here', \"don't\", 'will', \"mustn't\"])\n",
            "\n",
            " Nettoyage des phrases en cours...\n",
            "\n",
            " Phrase originale 1:\n",
            "   Maria Sharapova has basically no friends as tennis players on the WTA Tour....\n",
            " Apr√®s nettoyage:\n",
            "   maria sharapova basically friends tennis players wta tour\n",
            "\n",
            " Phrase originale 2:\n",
            "   The Russian player has no problems in openly speaking about it and in a recent interview she said: '...\n",
            " Apr√®s nettoyage:\n",
            "   russian player problems openly speaking recent interview said dont really hide feelings much\n",
            "\n",
            " Phrase originale 3:\n",
            "   I think everyone knows this is my job here....\n",
            " Apr√®s nettoyage:\n",
            "   think everyone knows job\n",
            "\n",
            "=== R√âSUM√â DU NETTOYAGE ===\n",
            " Phrases avant nettoyage: 128\n",
            " Phrases apr√®s nettoyage: 127\n",
            " Phrases √©limin√©es: 1\n",
            "\n",
            "=== APERCU DES PHRASES NETTOY√âES ===\n",
            "1. maria sharapova basically friends tennis players wta tour\n",
            "2. russian player problems openly speaking recent interview said dont really hide feelings much\n",
            "3. think everyone knows job\n",
            "4. courts court playing competitor want beat every single person whether theyre locker room across net\n",
            "5. one strike conversation weather know next minutes try win tennis match\n",
            "\n",
            "=== V√âRIFICATION DU VOCABULAIRE ===\n",
            " Mots uniques trouv√©s: 821/872\n",
            "Couverture GloVe: 94.2%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#√âtape 5 : Vectorisation des phrases\n",
        "L'objectif est de Convertir chaque phrase nettoy√©e en un vecteur num√©rique en utilisant les moyennes des embeddings GloVe."
      ],
      "metadata": {
        "id": "WMsTP3bsjLQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\" Vectorisation des phrases\")\n",
        "\n",
        "def sentence_to_vector(sentence, embeddings):\n",
        "    \"\"\"\n",
        "    Convertit une phrase en vecteur en moyennant les embeddings de ses mots.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): Phrase nettoy√©e\n",
        "        embeddings (dict): Dictionnaire des embeddings GloVe\n",
        "\n",
        "    Returns:\n",
        "        np.array: Vecteur de 100 dimensions, ou vecteur nul si aucun mot trouv√©\n",
        "    \"\"\"\n",
        "    words = sentence.split()\n",
        "    word_vectors = []\n",
        "\n",
        "    for word in words:\n",
        "        if word in embeddings:\n",
        "            word_vectors.append(embeddings[word])\n",
        "\n",
        "    if len(word_vectors) == 0:\n",
        "        # Retourne un vecteur nul si aucun mot n'est trouv√©\n",
        "        return np.zeros(100)\n",
        "\n",
        "    # Moyenne des vecteurs de mots\n",
        "    sentence_vector = np.mean(word_vectors, axis=0)\n",
        "    return sentence_vector\n",
        "\n",
        "# Application de la vectorisation\n",
        "sentence_vectors = []\n",
        "\n",
        "for idx, sentence in enumerate(cleaned_sentences):\n",
        "    vector = sentence_to_vector(sentence, word_embeddings)\n",
        "    sentence_vectors.append(vector)\n",
        "\n",
        "    # Affichage pour les premi√®res phrases\n",
        "    if idx < 3:\n",
        "        print(f\"\\n Phrase {idx+1}: {sentence}\")\n",
        "        print(f\" Vecteur (5 premi√®res dimensions): {vector[:5]}...\")\n",
        "        print(f\" Norme du vecteur: {np.linalg.norm(vector):.3f}\")\n",
        "\n",
        "# Conversion en array numpy pour des calculs plus efficaces\n",
        "sentence_vectors = np.array(sentence_vectors)\n",
        "\n",
        "# V√©rification finale\n",
        "print(\"\\n=== R√âSUM√â DE LA VECTORISATION ===\")\n",
        "print(f\"Nombre de phrases vectoris√©es: {len(sentence_vectors)}\")\n",
        "print(f\"Dimensions des vecteurs: {sentence_vectors.shape[1]}\")\n",
        "print(f\"Forme de la matrice: {sentence_vectors.shape}\")\n",
        "\n",
        "# Statistiques sur les vecteurs\n",
        "zero_vectors = np.sum(np.all(sentence_vectors == 0, axis=1))\n",
        "print(f\"Phrases sans vecteurs (mots non trouv√©s): {zero_vectors}\")\n",
        "\n",
        "# Visualisation de la distribution des normes\n",
        "norms = [np.linalg.norm(vec) for vec in sentence_vectors if not np.all(vec == 0)]\n",
        "if norms:\n",
        "    print(f\"Norme moyenne des vecteurs: {np.mean(norms):.3f}\")\n",
        "    print(f\"Norme min/maxe: {np.min(norms):.3f} / {np.max(norms):.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zc78EhtjVla",
        "outputId": "bb47470c-a6cf-493c-ffef-292f3bdb40d2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Vectorisation des phrases\n",
            "\n",
            " Phrase 1: maria sharapova basically friends tennis players wta tour\n",
            " Vecteur (5 premi√®res dimensions): [ 0.051489    0.1105585   0.6950863   0.18919174 -0.09581975]...\n",
            " Norme du vecteur: 3.802\n",
            "\n",
            " Phrase 2: russian player problems openly speaking recent interview said dont really hide feelings much\n",
            " Vecteur (5 premi√®res dimensions): [-0.07791846  0.19516078  0.41307408 -0.09757367 -0.26040584]...\n",
            " Norme du vecteur: 3.647\n",
            "\n",
            " Phrase 3: think everyone knows job\n",
            " Vecteur (5 premi√®res dimensions): [ 0.14818695  0.4246085   0.660479   -0.5043     -0.5471025 ]...\n",
            " Norme du vecteur: 4.765\n",
            "\n",
            "=== R√âSUM√â DE LA VECTORISATION ===\n",
            "Nombre de phrases vectoris√©es: 127\n",
            "Dimensions des vecteurs: 100\n",
            "Forme de la matrice: (127, 100)\n",
            "Phrases sans vecteurs (mots non trouv√©s): 0\n",
            "Norme moyenne des vecteurs: 3.571\n",
            "Norme min/maxe: 2.236 / 4.765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#√âtape 6 : Construction de la matrice de similarit√©\n",
        "\n",
        "l'Objectif est de cr√©er une matrice carr√©e o√π chaque cellule repr√©sente la similarit√© cosinus entre deux phrases."
      ],
      "metadata": {
        "id": "Spjq12tEj_Fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# √âtape 6 : Construction de la matrice de similarit√©\n",
        "print(\"Construction de la matrice de similarit√©...\")\n",
        "\n",
        "# Calcul de la matrice de similarit√© cosinus\n",
        "# cosine_similarity retourne une matrice de taille (n_phrases √ó n_phrases)\n",
        "similarity_matrix = cosine_similarity(sentence_vectors)\n",
        "\n",
        "# Affichage des dimensions\n",
        "print(f\"Dimensions de la matrice: {similarity_matrix.shape}\")\n",
        "print(f\"Type de donn√©es: {similarity_matrix.dtype}\")\n",
        "\n",
        "# V√©rification des valeurs\n",
        "print(f\"\\n=== STATISTIQUES DE LA MATRICE ===\")\n",
        "print(f\"Similarit√© min: {similarity_matrix.min():.3f}\")\n",
        "print(f\"Similarit√© max: {similarity_matrix.max():.3f}\")\n",
        "print(f\"Similarit√© moyenne: {similarity_matrix.mean():.3f}\")\n",
        "\n",
        "# Affichage d'un √©chantillon de la matrice\n",
        "print(\"\\n=== APERCU DE LA MATRICE (5√ó5) ===\")\n",
        "import pandas as pd\n",
        "sample_df = pd.DataFrame(\n",
        "    similarity_matrix[:5, :5],\n",
        "    index=[f\"P{i+1}\" for i in range(5)],\n",
        "    columns=[f\"P{i+1}\" for i in range(5)]\n",
        ")\n",
        "print(sample_df.round(3))\n",
        "\n",
        "# Visualisation de la diagonalit√© (chaque phrase est parfaitement similaire √† elle-m√™me)\n",
        "print(\"\\n=== V√âRIFICATION DIAGONALE ===\")\n",
        "diagonal_values = np.diagonal(similarity_matrix)\n",
        "print(f\"Valeurs diagonales (auto-similarit√©): {np.unique(diagonal_values)}\")\n",
        "\n",
        "# Identification des paires les plus similaires\n",
        "print(\"\\n=== PHRASES LES PLUS SIMILAIRES ===\")\n",
        "# Masque pour ignorer la diagonale\n",
        "mask = np.eye(similarity_matrix.shape[0], dtype=bool)\n",
        "masked_matrix = similarity_matrix.copy()\n",
        "masked_matrix[mask] = 0  # Met 0 sur la diagonale\n",
        "\n",
        "# Trouver les 3 paires les plus similaires\n",
        "flat_indices = np.argsort(masked_matrix.flatten())[-3:][::-1]\n",
        "for idx in flat_indices:\n",
        "    i, j = np.unravel_index(idx, similarity_matrix.shape)\n",
        "    sim_score = similarity_matrix[i, j]\n",
        "\n",
        "    print(f\"\\nSimilarit√©: {sim_score:.3f}\")\n",
        "    print(f\"Phrase {i+1}: {cleaned_sentences[i][:80]}...\")\n",
        "    print(f\"Phrase {j+1}: {cleaned_sentences[j][:80]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0yGRslVkAsJ",
        "outputId": "7afb4933-84ec-4b85-beb3-e51ab2779f0d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîó Construction de la matrice de similarit√©...\n",
            "üìä Dimensions de la matrice: (127, 127)\n",
            "üìà Type de donn√©es: float32\n",
            "\n",
            "=== STATISTIQUES DE LA MATRICE ===\n",
            "üìä Similarit√© min: 0.070\n",
            "üìä Similarit√© max: 1.000\n",
            "üìä Similarit√© moyenne: 0.733\n",
            "\n",
            "=== APERCU DE LA MATRICE (5√ó5) ===\n",
            "       P1     P2     P3     P4     P5\n",
            "P1  1.000  0.643  0.592  0.702  0.757\n",
            "P2  0.643  1.000  0.856  0.842  0.823\n",
            "P3  0.592  0.856  1.000  0.822  0.783\n",
            "P4  0.702  0.842  0.822  1.000  0.890\n",
            "P5  0.757  0.823  0.783  0.890  1.000\n",
            "\n",
            "=== V√âRIFICATION DIAGONALE ===\n",
            "Valeurs diagonales (auto-similarit√©): [0.9999996  0.9999997  0.99999976 0.9999998  0.9999999  0.99999994\n",
            " 1.         1.0000001  1.0000002  1.0000004 ]\n",
            "\n",
            "=== PHRASES LES PLUS SIMILAIRES ===\n",
            "\n",
            "Similarit√©: 0.957\n",
            "Phrase 62: think really nice environment great atmosphere especially veteran players helpin...\n",
            "Phrase 66: always really feel like mid years huge shift attitudes top players friendly givi...\n",
            "\n",
            "Similarit√©: 0.957\n",
            "Phrase 66: always really feel like mid years huge shift attitudes top players friendly givi...\n",
            "Phrase 62: think really nice environment great atmosphere especially veteran players helpin...\n",
            "\n",
            "Similarit√©: 0.953\n",
            "Phrase 66: always really feel like mid years huge shift attitudes top players friendly givi...\n",
            "Phrase 9: lot friends away courts said really close lot players something strategic...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#√âtape 7 : Construction du graphe et application de PageRank"
      ],
      "metadata": {
        "id": "3CmTCdiFktiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "# √âtape 7 : Construction du graphe et application de PageRank\n",
        "print(\"Construction du graphe de phrases...\")\n",
        "\n",
        "# Cr√©ation du graphe dirig√© pond√©r√©\n",
        "G = nx.Graph()  # Graphe non-dirig√© pour PageRank\n",
        "\n",
        "# Ajout des n≈ìuds (chaque phrase est un n≈ìud)\n",
        "for i in range(len(cleaned_sentences)):\n",
        "    G.add_node(i)\n",
        "\n",
        "# Ajout des ar√™tes pond√©r√©es par similarit√©\n",
        "# N'utilisons que les similarit√©s > 0.1 pour √©viter les connexions faibles\n",
        "threshold = 0.1 # Filtre les connexions faibles pour √©viter un graphe trop dense\n",
        "edges_added = 0\n",
        "\n",
        "for i in range(len(cleaned_sentences)):\n",
        "    for j in range(i+1, len(cleaned_sentences)):  # √âvite les doublons\n",
        "        if similarity_matrix[i][j] > threshold:\n",
        "            G.add_edge(i, j, weight=similarity_matrix[i][j])\n",
        "            edges_added += 1\n",
        "\n",
        "print(f\" N≈ìuds dans le graphe: {G.number_of_nodes()}\")\n",
        "print(f\"Ar√™tes ajout√©es: {edges_added}\")\n",
        "\n",
        "# Application de l'algorithme PageRank\n",
        "print(\"\\nCalcul du PageRank...\")\n",
        "pagerank_scores = nx.pagerank(\n",
        "    G,\n",
        "    weight='weight',  # Utilise les poids de similarit√©\n",
        "    max_iter=100,     # Maximum d'it√©rations\n",
        "    tol=1e-06         # Tol√©rance de convergence\n",
        ")\n",
        "\n",
        "# Affichage des scores\n",
        "print(\"\\n=== SCORES PAGERANK (TOP 10) ===\")\n",
        "# Tri par score d√©croissant\n",
        "sorted_scores = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for rank, (idx, score) in enumerate(sorted_scores[:10], 1):\n",
        "    print(f\"\\nRang {rank} - Score: {score:.4f}\")\n",
        "    print(f\"Phrase: {cleaned_sentences[idx][:100]}...\")\n",
        "\n",
        "# Distribution des scores\n",
        "print(\"\\n=== DISTRIBUTION DES SCORES ===\")\n",
        "scores = list(pagerank_scores.values())\n",
        "print(f\"Score moyen: {np.mean(scores):.4f}\")\n",
        "print(f\"Score m√©dian: {np.median(scores):.4f}\")\n",
        "print(f\"Score min/max: {min(scores):.4f} / {max(scores):.4f}\")\n",
        "\n",
        "# Visualisation rapide du graphe (optionnel)\n",
        "print(\"\\n=== STRUCTURE DU GRAPHE ===\")\n",
        "print(f\"Nombre de composantes connexes: {nx.number_connected_components(G)}\")\n",
        "print(f\"Densit√© du graphe: {nx.density(G):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbIntqj8k--a",
        "outputId": "22292ed6-5e6a-4325-eced-05380915d655"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Construction du graphe de phrases...\n",
            " N≈ìuds dans le graphe: 127\n",
            "Ar√™tes ajout√©es: 8000\n",
            "\n",
            "Calcul du PageRank...\n",
            "\n",
            "=== SCORES PAGERANK (TOP 10) ===\n",
            "\n",
            "Rang 1 - Score: 0.0088\n",
            "Phrase: nice trajectorythen reid recalledif hadnt got sick think could started pushing towards second week s...\n",
            "\n",
            "Rang 2 - Score: 0.0087\n",
            "Phrase: major players feel big event late november combined one january australian open mean much tennis lit...\n",
            "\n",
            "Rang 3 - Score: 0.0087\n",
            "Phrase: one strike conversation weather know next minutes try win tennis match...\n",
            "\n",
            "Rang 4 - Score: 0.0087\n",
            "Phrase: felt like best weeks get know players playing fed cup weeks olympic weeks necessarily tournaments...\n",
            "\n",
            "Rang 5 - Score: 0.0086\n",
            "Phrase: speaking swiss indoors tournament play sundays final romanian qualifier marius copil world number th...\n",
            "\n",
            "Rang 6 - Score: 0.0086\n",
            "Phrase: felt like really kind changed people little bit definitely lot quiet started become better meanwhile...\n",
            "\n",
            "Rang 7 - Score: 0.0086\n",
            "Phrase: exhausted spending half round deep bushes searching ball well two golfers hed never met incredibly g...\n",
            "\n",
            "Rang 8 - Score: 0.0086\n",
            "Phrase: used first break point close first set going second wrapping win first match point...\n",
            "\n",
            "Rang 9 - Score: 0.0086\n",
            "Phrase: federer said earlier month shanghai chances playing davis cup nonexistent...\n",
            "\n",
            "Rang 10 - Score: 0.0085\n",
            "Phrase: federer easier time previous match medvedev threesetter shanghai two weeks ago...\n",
            "\n",
            "=== DISTRIBUTION DES SCORES ===\n",
            "Score moyen: 0.0079\n",
            "Score m√©dian: 0.0080\n",
            "Score min/max: 0.0053 / 0.0088\n",
            "\n",
            "=== STRUCTURE DU GRAPHE ===\n",
            "Nombre de composantes connexes: 1\n",
            "Densit√© du graphe: 0.9999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# √âtape 8 : R√©sum√© final\n",
        "\n",
        "L'Objectif est d'extraire les phrases les plus importantes selon PageRank pour cr√©er un r√©sum√© coh√©rent."
      ],
      "metadata": {
        "id": "7El0ix0qmBXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# R√©sum√© final\n",
        "\n",
        "\n",
        "# Param√®tres du r√©sum√©\n",
        "NUM_SENTENCES_SUMMARY = 5  # Nombre de phrases dans le r√©sum√©\n",
        "\n",
        "# R√©cup√©ration des phrases originales (non nettoy√©es) pour le r√©sum√©\n",
        "# Important : on utilise les phrases originales pour garder la lisibilit√©\n",
        "original_sentences = [s for s in all_sentences if s.strip()]\n",
        "\n",
        "# Cr√©ation du mapping entre indices de phrases nettoy√©es et originales\n",
        "# (Dans notre cas, cleaned_sentences et original_sentences correspondent)\n",
        "phrase_mapping = {i: i for i in range(len(original_sentences))}\n",
        "\n",
        "# R√©cup√©ration des meilleures phrases\n",
        "top_indices = [idx for idx, score in sorted_scores[:NUM_SENTENCES_SUMMARY]]\n",
        "\n",
        "# Tri par ordre d'apparition dans les articles originaux\n",
        "# (Pour garder la coh√©rence narrative)\n",
        "top_indices_sorted = sorted(top_indices)\n",
        "\n",
        "# Construction du r√©sum√©\n",
        "summary_sentences = [original_sentences[idx] for idx in top_indices_sorted]\n",
        "\n",
        "# Affichage du r√©sum√©\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"R√âSUM√â\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, sentence in enumerate(summary_sentences, 1):\n",
        "    # Nettoyage de l'affichage (suppression des espaces multiples)\n",
        "    clean_display = ' '.join(sentence.split())\n",
        "    print(f\"\\n{i}. {clean_display}\")\n",
        "\n",
        "# Statistiques du r√©sum√©\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STATISTIQUES DU R√âSUM√â\")\n",
        "print(\"=\"*60)\n",
        "print(f\"üìÑ Nombre de phrases: {len(summary_sentences)}\")\n",
        "print(f\"üìä Score moyen PageRank des phrases s√©lectionn√©es: {np.mean([pagerank_scores[idx] for idx in top_indices]):.4f}\")\n",
        "print(f\"üìè Longueur moyenne des phrases: {np.mean([len(s.split()) for s in summary_sentences]):.1f} mots\")\n",
        "\n",
        "# Calcul du taux de compression\n",
        "original_length = sum(len(s.split()) for s in original_sentences)\n",
        "summary_length = sum(len(s.split()) for s in summary_sentences)\n",
        "compression_rate = (1 - summary_length/original_length) * 100\n",
        "\n",
        "print(f\"üìà Taux de compression: {compression_rate:.1f}%\")\n",
        "print(f\"   Texte original: {original_length} mots\")\n",
        "print(f\"   R√©sum√©: {summary_length} mots\")\n",
        "\n",
        "# Sauvegarde optionnelle du r√©sum√©\n",
        "print(\"\\nSauvegarde du r√©sum√©...\")\n",
        "summary_text = \"\\n\\n\".join(summary_sentences)\n",
        "with open(\"/content/tennis_summary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"R√âSUM√â DES ARTICLES DE TENNIS\\n\")\n",
        "    f.write(\"=\"*50 + \"\\n\\n\")\n",
        "    f.write(summary_text)\n",
        "\n",
        "print(\" R√©sum√© sauvegard√© dans 'tennis_summary.txt'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7PSsqCdmfmy",
        "outputId": "6b237533-e26d-4284-a5b0-4bcc4d9dc687"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "R√âSUM√â\n",
            "============================================================\n",
            "\n",
            "1. So I'm not the one to strike up a conversation about the weather and know that in the next few minutes I have to go and try to win a tennis match.\n",
            "\n",
            "2. Roger Federer has revealed that organisers of the re-launched and condensed Davis Cup gave him three days to decide if he would commit to the controversial competition.\n",
            "\n",
            "3. Novak Djokovic has said he will give precedence to the ATP¬ís intended re-launch of the defunct World Team Cup in January 2020, at various Australian venues.\n",
            "\n",
            "4. ¬ìIt's a very pleasant atmosphere, I'd have to say, around the locker rooms.\n",
            "\n",
            "5. He¬íd backed up his last-32 showingat Melbourne Park with a string of wins over elites including French Open champion and then world No.9 Gaston Gaudio and Roland Garros runner-up Martin Verkerk in 2004 before illness struck.\n",
            "\n",
            "============================================================\n",
            "STATISTIQUES DU R√âSUM√â\n",
            "============================================================\n",
            "üìÑ Nombre de phrases: 5\n",
            "üìä Score moyen PageRank des phrases s√©lectionn√©es: 0.0087\n",
            "üìè Longueur moyenne des phrases: 26.8 mots\n",
            "üìà Taux de compression: 95.1%\n",
            "   Texte original: 2755 mots\n",
            "   R√©sum√©: 134 mots\n",
            "\n",
            "Sauvegarde du r√©sum√©...\n",
            " R√©sum√© sauvegard√© dans 'tennis_summary.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion :\n",
        "Le syst√®me de r√©sum√© automatique est COMPLET !\n",
        "Le pipeline NLP  est construit de A √† Z :\n",
        "\n",
        "R√©capitulatif des √©tapes suivies :\n",
        "\n",
        "\n",
        "| √âtape   | Comp√©tence acquise                         | Statut     |\n",
        "| ------- | ------------------------------------------ | ---------- |\n",
        "| ‚úÖ **1** | Chargement et exploration des donn√©es      | **R√©ussi** |\n",
        "| ‚úÖ **2** | Tokenisation avanc√©e avec NLTK             | **R√©ussi** |\n",
        "| ‚úÖ **3** | Int√©gration d'embeddings GloVe (400K mots) | **R√©ussi** |\n",
        "| ‚úÖ **4** | Nettoyage NLP avec stopwords et regex      | **R√©ussi** |\n",
        "| ‚úÖ **5** | Vectorisation par moyenne d'embeddings     | **R√©ussi** |\n",
        "| ‚úÖ **6** | Calcul de similarit√© cosinus (127√ó127)     | **R√©ussi** |\n",
        "| ‚úÖ **7** | Graph-based ranking avec PageRank          | **R√©ussi** |\n",
        "| ‚úÖ **8** | G√©n√©ration de r√©sum√© coh√©rent              | **R√©ussi** |\n"
      ],
      "metadata": {
        "id": "lozaFwepnpLb"
      }
    }
  ]
}