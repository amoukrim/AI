{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPy5NVsxMyJizQO5a0UfcC1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amoukrim/AI/blob/main/Week8/DailyChallenge/dailyChallengew_8_d5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##@Author : Adil MOUKRIM\n",
        "\n",
        "Building an agent with LangGraph\n",
        "Last Updated: March 28th, 2025\n",
        "\n",
        "## Daily Challenge : Building an agent with LangGraph and the Gemini API\n",
        "\n",
        "\n",
        "üë©‚Äçüè´ üë©üèø‚Äçüè´ What You‚Äôll learn\n",
        "How to create a stateful application using LangGraph.\n",
        "How to integrate Gemini API (via LangChain) into your LangGraph application.\n",
        "How to define and manipulate state using TypedDict and node functions.\n",
        "How to simulate dynamic, tool-augmented behavior (like menus and ordering) in a conversational loop.\n",
        "How to model conditional transitions, loops, and user interaction using LangGraph.\n",
        "\n",
        "\n",
        "üõ†Ô∏è What you will create\n",
        "A conversational cafe ordering system called BaristaBot. This bot will:\n",
        "\n",
        "Use natural language to take coffee/tea orders.\n",
        "Offer a real-time menu using a tool.\n",
        "Confirm and modify orders.\n",
        "Loop through conversation until an order is placed.\n",
        "Handle tool calls using LangGraph‚Äôs ToolNode mechanism.\n",
        "This graph-based app will simulate a real-world cafe assistant using AI + state management.\n",
        "\n",
        "\n",
        "\n",
        "Task\n",
        "In this notebook, you will use LangGraph to define a stateful graph-based application built on top of the Gemini API.\n",
        "You will build a simulated cafe ordering system, called BaristaBot. It will provide a looping chat interface to customers where they can order cafe beverages using natural language, and you will build nodes to represent the cafe‚Äôs live menu and the ‚Äúback room‚Äù ordering system.\n",
        "BaristaBot is used in other Gemini API demos, so if you are looking to explore something with a more minimal implementation, check out the BaristaBot function calling example that implements a similar system using only the Gemini API Python SDK and function calling.\n",
        "\n",
        "1. Start by installing and importing the LangGraph SDK and LangChain support for the Gemini API %pip install -qU 'langgraph==0.2.45' 'langchain-google-genai==2.0.4'.\n",
        "\n",
        "\n",
        "\n",
        "2. Set up your API key : The GOOGLE_API_KEY environment variable can be set to automatically configure the underlying API. This works for both the official Gemini Python SDK and for LangChain/LangGraph.\n",
        "\n",
        "To run the following cell, your API key must be stored it in a Kaggle secret named GOOGLE_API_KEY.\n",
        "If you don‚Äôt already have an API key, you can grab one from AI Studio. You can find detailed instructions in the docs.\n",
        "To make the key available through Kaggle secrets, choose Secrets from the Add-ons menu and follow the instructions to add your key or enable it for this notebook.\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "\n",
        "\n",
        "LangGraph applications are built around a graph structure. As the developer, you define an application graph that models the state transitions for your application. Your app will define a state schema, and an instance of that schema is propagated through the graph.\n",
        "\n",
        "Each node in the graph represents an action or step that can be taken. Nodes will make changes to the state in some way through code that you define. These changes can be the result of invoking an LLM, by calling an API, or executing any logic that the node defines.\n",
        "\n",
        "Each edge in the graph represents a transition between states, defining the flow of the program. Edge transitions can be fixed, for example if you define a text-only chatbot where output is always displayed to a user, you may always transition from chatbot -> user. The transitions can also be conditional, allowing you to add branching (like an if-else statement) or looping (like for or while loops).\n",
        "\n",
        "\n",
        "\n",
        "3. Define core instructions: State is a fundamental concept for a LangGraph app. A state object is passed between every node and transition in the app. Here you define a state object, OrderState, that holds the conversation history, a structured order, and a flag indicating if the customer has finished placing their order. For simplicity, the ‚Äústructure‚Äù in this order is just a\n",
        "list of strings, but this can be expanded to any Python data structure.\n",
        "\n",
        "In Python, the LangGraph state object is a Python dictionary. You can provide a schema for this dictionary by defining it as a TypedDict.\n",
        "Here you also define the system instruction that the Gemini model will use. You can capture tone and style here, as well as the playbook under which the chatbot should operate.\n",
        "\n",
        "\n",
        "\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "class OrderState(TypedDict):\n",
        "    \"\"\"State representing the customer's order conversation.\"\"\"\n",
        "\n",
        "    # The chat conversation. This preserves the conversation history\n",
        "    # between nodes. The `add_messages` annotation indicates to LangGraph\n",
        "    # that state is updated by appending returned messages, not replacing\n",
        "    # them.\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "    # The customer's in-progress order.\n",
        "    order: list[str]\n",
        "\n",
        "    # Flag indicating that the order is placed and completed.\n",
        "    finished: bool\n",
        "# The system instruction defines how the chatbot is expected to behave and includes\n",
        "# rules for when to call different functions, as well as rules for the conversation, such\n",
        "# as tone and what is permitted for discussion.\n",
        "BARISTABOT_SYSINT = (\n",
        "    \"system\",  # 'system' indicates the message is a system instruction.\n",
        "    \"You are a BaristaBot, an interactive cafe ordering system. A human will talk to you about the \"\n",
        "    \"available products you have and you will answer any questions about menu items (and only about \"\n",
        "    \"menu items - no off-topic discussion, but you can chat about the products and their history). \"\n",
        "    \"The customer will place an order for 1 or more items from the menu, which you will structure \"\n",
        "    \"and send to the ordering system after confirming the order with the human. \"\n",
        "    \"\\n\\n\"\n",
        "    \"Add items to the customer's order with add_to_order, and reset the order with clear_order. \"\n",
        "    \"To see the contents of the order so far, call get_order (this is shown to you, not the user) \"\n",
        "    \"Always confirm_order with the user (double-check) before calling place_order. Calling confirm_order will \"\n",
        "    \"display the order items to the user and returns their response to seeing the list. Their response may contain modifications. \"\n",
        "    \"Always verify and respond with drink and modifier names from the MENU before adding them to the order. \"\n",
        "    \"If you are unsure a drink or modifier matches those on the MENU, ask a question to clarify or redirect. \"\n",
        "    \"You only have the modifiers listed on the menu. \"\n",
        "    \"Once the customer has finished ordering items, Call confirm_order to ensure it is correct then make \"\n",
        "    \"any necessary updates and then call place_order. Once place_order has returned, thank the user and \"\n",
        "    \"say goodbye!\",\n",
        ")\n",
        "\n",
        "# This is the message with which the system opens the conversation.\n",
        "WELCOME_MSG = \"Welcome to the BaristaBot cafe. Type `q` to quit. How may I serve you today?\"\n",
        "\n",
        "\n",
        "4. Define a single turn chatboot\n",
        "\n",
        "To illustrate how LangGraph works, the following program defines a chatbot node that will execute a single turn in a chat conversation using the instructions supplied.\n",
        "\n",
        "Each node in the graph operates on the state object. The state (a Python dictionary) is passed as a parameter into the node (a function) and the new state is returned. This can be restated as pseudo-code, where state = node(state).\n",
        "\n",
        "Note: For the chatbot node, the state is updated by adding the new conversation message. The add_messages annotation on OrderState.messages indicates that messages are appended when returned from a node. Typically state is updated by replacement, but this annotation causes messages to behave differently.\n",
        "\n",
        "\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Try using different models. The `pro` models perform the best, especially\n",
        "# with tool-calling. The `flash` models are super fast, and are a good choice\n",
        "# if you need to use the higher free-tier quota.\n",
        "# Check out the features and quota differences here: https://ai.google.dev/pricing\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n",
        "\n",
        "\n",
        "def chatbot(state: OrderState) -> OrderState:\n",
        "    \"\"\"The chatbot itself. A simple wrapper around the model's own chat interface.\"\"\"\n",
        "    message_history = [BARISTABOT_SYSINT] + state[\"messages\"]\n",
        "    return {\"messages\": [llm.invoke(message_history)]}\n",
        "\n",
        "\n",
        "# Set up the initial graph based on our state definition.\n",
        "graph_builder = ...\n",
        "\n",
        "# Add the chatbot function to the app graph as a node called \"chatbot\".\n",
        "graph_builder.add_node(...)\n",
        "\n",
        "# Define the chatbot node as the app entrypoint.\n",
        "graph_builder.add_edge(...)\n",
        "\n",
        "chat_graph = ...\n",
        "\n",
        "\n",
        "5. Visualise the graph you just defined.\n",
        "\n",
        "\n",
        "\n",
        "6. Now that the graph is defined, you can run it. It only has one node, and one transition into that node, so it will transition from start to chatbot, execute the chatbot node, and terminate.\n",
        "Run the graph by calling invoke and passing an initial state object. In this case it begins with the user‚Äôs initial message.\n",
        "\n",
        "\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "user_msg = ...\n",
        "state = ...\n",
        "\n",
        "# The state object contains lots of information. Uncomment the pprint lines to see it all.\n",
        "# pprint(state)\n",
        "\n",
        "# Note that the final state now has 2 messages. Our HumanMessage, and an additional AIMessage.\n",
        "for msg in state[\"messages\"]:\n",
        "    print(f\"{type(msg).__name__}: {msg.content}\")\n",
        "\n",
        "\n",
        "7. You could execute this in a Python loop, but for simplicity, manually invoke one more conversational turn. This second invocation takes the state from the first call and appends another user message to elicit another response from the chatbot.\n",
        "\n",
        "\n",
        "\n",
        "user_msg = ...\n",
        "\n",
        "state[\"messages\"].append(...)\n",
        "state = chat_graph.invoke(...)\n",
        "\n",
        "# pprint(state)\n",
        "for msg in state[\"messages\"]:\n",
        "    print(f\"{type(msg).__name__}: {msg.content}\")\n",
        "\n",
        "\n",
        "8. Add a human node : Instead of repeatedly running the ‚Äúgraph‚Äù in a Python loop, you can use LangGraph to loop between nodes.\n",
        "\n",
        "The human node will display the last message from the LLM to the user, and then prompt them for their next input. Here this is done using standard Python print and input functions, but for a real cafe situation, you could render the chat to a display or audio, and accept input from a mic or on-screen keyboard.\n",
        "\n",
        "The chatbot node function has also been updated to include the welcome message to start the conversation. Complete the following code :\n",
        "\n",
        "\n",
        "\n",
        "from langchain_core.messages.ai import AIMessage\n",
        "\n",
        "\n",
        "def human_node(state: OrderState) -> OrderState:\n",
        "    \"\"\"Display the last model message to the user, and receive the user's input.\"\"\"\n",
        "    last_msg = state[\"messages\"][-1]\n",
        "    print(\"Model:\", last_msg.content)\n",
        "\n",
        "    user_input = input(\"User: \")\n",
        "\n",
        "    # If it looks like the user is trying to quit, flag the conversation\n",
        "    # as over.\n",
        "    if user_input in {\"q\", \"quit\", \"exit\", \"goodbye\"}:\n",
        "        state[\"finished\"] = True\n",
        "\n",
        "    return state | {\"messages\": [(\"user\", user_input)]}\n",
        "\n",
        "\n",
        "def chatbot_with_welcome_msg(state: OrderState) -> OrderState:\n",
        "    \"\"\"The chatbot itself. A wrapper around the model's own chat interface.\"\"\"\n",
        "\n",
        "    if state[\"messages\"]:\n",
        "        # If there are messages, continue the conversation with the Gemini model.\n",
        "        new_output = llm.invoke([BARISTABOT_SYSINT] + state[\"messages\"])\n",
        "    else:\n",
        "        # If there are no messages, start with the welcome message.\n",
        "        new_output = AIMessage(content=WELCOME_MSG)\n",
        "\n",
        "    return state | {\"messages\": [new_output]}\n",
        "\n",
        "\n",
        "# Start building a new graph.\n",
        "graph_builder = ...\n",
        "\n",
        "# Add the chatbot and human nodes to the app graph.\n",
        "graph_builder.add_node(...)\n",
        "graph_builder.add_node(...)\n",
        "\n",
        "# Start with the chatbot again.\n",
        "graph_builder.add_edge(...)\n",
        "\n",
        "# The chatbot will always go to the human next.\n",
        "graph_builder.add_edge(...);\n",
        "\n",
        "\n",
        "9. Before you can run this, note that if you added an edge from human back to chatbot, the graph will cycle forever as there is no exit condition. One way to break the cycle is to add a check for a human input like q or quit and use that to break the loop.\n",
        "\n",
        "In LangGraph, this is achieved with a conditional edge. This is similar to a regular graph transition, except a custom function is called to determine which edge to traverse.\n",
        "\n",
        "Conditional edge functions take the state as input, and return a string representing the name of the node to which it will transition. Complete this code:\n",
        "\n",
        "\n",
        "\n",
        "from typing import Literal\n",
        "\n",
        "\n",
        "def maybe_exit_human_node(state: OrderState) -> Literal[\"chatbot\", \"__end__\"]:\n",
        "    \"\"\"Route to the chatbot, unless it looks like the user is exiting.\"\"\"\n",
        "    if state.get(\"finished\", False):\n",
        "        return END\n",
        "    else:\n",
        "        return ...\n",
        "\n",
        "\n",
        "graph_builder.add_conditional_edges(...)\n",
        "\n",
        "chat_with_human_graph = ...\n",
        "\n",
        "Image(chat_with_human_graph.get_graph().draw_mermaid_png())\n",
        "\n",
        "\n",
        "10. Add a ‚Äúlive‚Äù menu. BaristaBot currently has no awareness of the available items at the cafe, so it will hallucinate a menu. One option would be to hard-code a menu into the system prompt. This would work well, but to simulate a system where the menu is more dynamic and could respond to fluctuating stock levels, you will put the menu into a custom tool.\n",
        "\n",
        "There are two types of tools that this system will use. Stateless tools that can be run automatically, and stateful tools that modify the order. The ‚Äúget current menu‚Äù tool is stateless, in that it does not make any changes to the live order, so it can be called automatically.\n",
        "\n",
        "In a LangGraph app, you can annotate Python functions as tools by applying the @tools annotation.\n",
        "\n",
        "\n",
        "\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "\n",
        "@tool\n",
        "def get_menu() -> str:\n",
        "    \"\"\"Provide the latest up-to-date menu.\"\"\"\n",
        "    # Note that this is just hard-coded text, but you could connect this to a live stock\n",
        "    # database, or you could use Gemini's multi-modal capabilities and take live photos of\n",
        "    # your cafe's chalk menu or the products on the counter and assmble them into an input.\n",
        "\n",
        "    return \"\"\"\n",
        "    MENU:\n",
        "    Coffee Drinks:\n",
        "    Espresso\n",
        "    Americano\n",
        "    Cold Brew\n",
        "\n",
        "    Coffee Drinks with Milk:\n",
        "    Latte\n",
        "    Cappuccino\n",
        "    Cortado\n",
        "    Macchiato\n",
        "    Mocha\n",
        "    Flat White\n",
        "\n",
        "    Tea Drinks:\n",
        "    English Breakfast Tea\n",
        "    Green Tea\n",
        "    Earl Grey\n",
        "\n",
        "    Tea Drinks with Milk:\n",
        "    Chai Latte\n",
        "    Matcha Latte\n",
        "    London Fog\n",
        "\n",
        "    Other Drinks:\n",
        "    Steamer\n",
        "    Hot Chocolate\n",
        "Modifiers:\n",
        "    Milk options: Whole, 2%, Oat, Almond, 2% Lactose Free; Default option: whole\n",
        "    Espresso shots: Single, Double, Triple, Quadruple; default: Double\n",
        "    Caffeine: Decaf, Regular; default: Regular\n",
        "    Hot-Iced: Hot, Iced; Default: Hot\n",
        "    Sweeteners (option to add one or more): vanilla sweetener, hazelnut sweetener, caramel sauce, chocolate sauce, sugar free vanilla sweetener\n",
        "    Special requests: any reasonable modification that does not involve items not on the menu, for example: 'extra hot', 'one pump', 'half caff', 'extra foam', etc.\n",
        "\n",
        "    \"dirty\" means add a shot of espresso to a drink that doesn't usually have it, like \"Dirty Chai Latte\".\n",
        "    \"Regular milk\" is the same as 'whole milk'.\n",
        "    \"Sweetened\" means add some regular sugar, not a sweetener.\n",
        "\n",
        "    Soy milk has run out of stock today, so soy is not available.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "Now add the new tool to the graph. The get_menu tool is wrapped in a ToolNode that handles calling the tool and passing the response as a message through the graph. The tools are also bound to the llm object so that the underlying model knows they exist. As you now have a different llm object to invoke, you need to update the chatbot node so that it is aware of the tools.\n",
        "\n",
        "\n",
        "\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "\n",
        "# Define the tools and create a \"tools\" node.\n",
        "tools = [get_menu]\n",
        "tool_node = ToolNode(tools)\n",
        "\n",
        "# Attach the tools to the model so that it knows what it can call.\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "\n",
        "def maybe_route_to_tools(state: OrderState) -> Literal[\"tools\", \"human\"]:\n",
        "    \"\"\"Route between human or tool nodes, depending if a tool call is made.\"\"\"\n",
        "    if not (msgs := state.get(\"messages\", [])):\n",
        "        raise ValueError(f\"No messages found when parsing state: {state}\")\n",
        "\n",
        "    # Only route based on the last message.\n",
        "    msg = msgs[-1]\n",
        "\n",
        "    # When the chatbot returns tool_calls, route to the \"tools\" node.\n",
        "    if hasattr(msg, \"tool_calls\") and len(msg.tool_calls) > 0:\n",
        "        return \"tools\"\n",
        "    else:\n",
        "        return \"human\"\n",
        "\n",
        "def chatbot_with_tools(state: OrderState) -> OrderState:\n",
        "    \"\"\"The chatbot with tools. A simple wrapper around the model's own chat interface.\"\"\"\n",
        "    defaults = {\"order\": [], \"finished\": False}\n",
        "\n",
        "    if state[\"messages\"]:\n",
        "        new_output = llm_with_tools.invoke([BARISTABOT_SYSINT] + state[\"messages\"])\n",
        "    else:\n",
        "        new_output = AIMessage(content=WELCOME_MSG)\n",
        "\n",
        "    # Set up some defaults if not already set, then pass through the provided state,\n",
        "    # overriding only the \"messages\" field.\n",
        "    return defaults | state | {\"messages\": [new_output]}\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(OrderState)\n",
        "\n",
        "# Add the nodes, including the new tool_node.\n",
        "graph_builder.add_node(\"chatbot\", chatbot_with_tools)\n",
        "graph_builder.add_node(\"human\", human_node)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "# Chatbot may go to tools, or human.\n",
        "graph_builder.add_conditional_edges(\"chatbot\", maybe_route_to_tools)\n",
        "# Human may go back to chatbot, or exit.\n",
        "graph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)\n",
        "\n",
        "# Tools always route back to chat afterwards.\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_with_menu = graph_builder.compile()\n",
        "\n",
        "Image(graph_with_menu.get_graph().draw_mermaid_png())\n",
        "\n",
        "\n",
        "\n",
        "11. Handle orders. To build up an order during the chat conversation, you will need to update the state to track the order, and provide simple tools that update this state. These need to be explicit as the model should not directly have access to the apps internal state, or it risks being manipulated arbitrarily.\n",
        "\n",
        "The ordering tools will be added as stubs in a separate node so that you can edit the state directly. Using the @tool annotation is still a handy way to define their schema, so the ordering tools below are implemented as empty Python functions.\n",
        "Complete this code :\n",
        "\n",
        "\n",
        "\n",
        "from collections.abc import Iterable\n",
        "from random import randint\n",
        "\n",
        "from langgraph.prebuilt import InjectedState\n",
        "from langchain_core.messages.tool import ToolMessage\n",
        "\n",
        "# These functions have no body; LangGraph does not allow @tools to update\n",
        "# the conversation state, so you will implement a separate node to handle\n",
        "# state updates. Using @tools is still very convenient for defining the tool\n",
        "# schema, so empty functions have been defined that will be bound to the LLM\n",
        "# but their implementation is deferred to the order_node.\n",
        "\n",
        "\n",
        "@tool\n",
        "def add_to_order(drink: str, modifiers: Iterable[str]) -> str:\n",
        "    \"\"\"Adds the specified drink to the customer's order, including any modifiers.\n",
        "\n",
        "    Returns:\n",
        "      The updated order in progress.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "@tool\n",
        "def confirm_order() -> str:\n",
        "    \"\"\"Asks the customer if the order is correct.\n",
        "\n",
        "    Returns:\n",
        "      The user's free-text response.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "@tool\n",
        "def get_order() -> str:\n",
        "    \"\"\"Returns the users order so far. One item per line.\"\"\"@tool\n",
        "def clear_order():\n",
        "    \"\"\"Removes all items from the user's order.\"\"\"\n",
        "\n",
        "\n",
        "@tool\n",
        "def place_order() -> int:\n",
        "    \"\"\"Sends the order to the barista for fulfillment.\n",
        "\n",
        "    Returns:\n",
        "      The estimated number of minutes until the order is ready.\n",
        "    \"\"\"\n",
        "\n",
        "def order_node(state: OrderState) -> OrderState:\n",
        "    \"\"\"The ordering node. This is where the order state is manipulated.\"\"\"\n",
        "    tool_msg = ...\n",
        "    order = ...\n",
        "    outbound_msgs = []\n",
        "    order_placed = False\n",
        "\n",
        "    for tool_call in tool_msg.tool_calls:\n",
        "\n",
        "        if tool_call[\"name\"] == \"add_to_order\":\n",
        "\n",
        "            # Each order item is just a string. This is where it assembled as \"drink (modifiers, ...)\".\n",
        "            modifiers = ...\n",
        "            modifier_str = \", \".join(modifiers) if modifiers else \"no modifiers\"\n",
        "\n",
        "            order.append(f'{tool_call[\"args\"][\"drink\"]} ({modifier_str})')\n",
        "            response = \"\\n\".join(order)\n",
        "\n",
        "        elif tool_call[\"name\"] == ...;\n",
        "\n",
        "            # We could entrust the LLM to do order confirmation, but it is a good practice to\n",
        "            # show the user the exact data that comprises their order so that what they confirm\n",
        "            # precisely matches the order that goes to the kitchen - avoiding hallucination\n",
        "            # or reality skew.\n",
        "\n",
        "            # In a real scenario, this is where you would connect your POS screen to show the\n",
        "            # order to the user.\n",
        "\n",
        "            print(\"Your order:\")\n",
        "            if not order:\n",
        "                print(\"  (no items)\")\n",
        "\n",
        "            for drink in order:\n",
        "                print(f\"  {drink}\")\n",
        "\n",
        "            response = input(\"Is this correct? \")\n",
        "\n",
        "        elif tool_call[\"name\"] == \"get_order\":\n",
        "\n",
        "            response = \"\\n\".join(order) if order else \"(no order)\"\n",
        "\n",
        "        elif tool_call[\"name\"] == \"clear_order\":\n",
        "\n",
        "            order.clear()response = None\n",
        "\n",
        "        elif tool_call[\"name\"] == \"place_order\":\n",
        "\n",
        "            order_text = \"\\n\".join(order)\n",
        "            print(\"Sending order to kitchen!\")\n",
        "            print(order_text)\n",
        "\n",
        "            # TODO(you!): Implement cafe.\n",
        "            order_placed = True\n",
        "            response = randint(1, 5)  # ETA in minutes\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError(f'Unknown tool call: {tool_call[\"name\"]}')\n",
        "\n",
        "        # Record the tool results as tool messages.\n",
        "        outbound_msgs.append(\n",
        "            ToolMessage(\n",
        "                content=response,\n",
        "                name=tool_call[\"name\"],\n",
        "                tool_call_id=tool_call[\"id\"],\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return {\"messages\": outbound_msgs, \"order\": order, \"finished\": order_placed}\n",
        "\n",
        "def maybe_route_to_tools(state: OrderState) -> str:\n",
        "    \"\"\"Route between chat and tool nodes if a tool call is made.\"\"\"\n",
        "    if not (msgs := state.get(\"messages\", [])):\n",
        "        raise ValueError(f\"No messages found when parsing state: {state}\")\n",
        "\n",
        "    msg = msgs[-1]\n",
        "\n",
        "    if state.get(\"finished\", False):\n",
        "        # When an order is placed, exit the app. The system instruction indicates\n",
        "        # that the chatbot should say thanks and goodbye at this point, so we can exit\n",
        "        # cleanly.\n",
        "        return END\n",
        "\n",
        "    elif hasattr(msg, \"tool_calls\") and len(msg.tool_calls) > 0:\n",
        "        # Route to `tools` node for any automated tool calls first.\n",
        "        if any(\n",
        "            tool[\"name\"] in tool_node.tools_by_name.keys() for tool in msg.tool_calls\n",
        "        ):\n",
        "            return \"tools\"\n",
        "        else:\n",
        "            return \"ordering\"\n",
        "\n",
        "    else:\n",
        "        return \"human\"\n",
        "\n",
        "\n",
        "12. Now define the graph. The LLM needs to know about the tools too, so that it can invoke them. Here you set up 2 sets of tools corresponding to the nodes under which they operate: automated and ordering.\n",
        "\n",
        "\n",
        "\n",
        "# Auto-tools will be invoked automatically by the ToolNode\n",
        "auto_tools = [get_menu]\n",
        "tool_node = ...\n",
        "\n",
        "# Order-tools will be handled by the order node.\n",
        "order_tools = [...]\n",
        "\n",
        "# The LLM needs to know about all of the tools, so specify everything here.\n",
        "llm_with_tools = ...\n",
        "\n",
        "\n",
        "graph_builder = ...\n",
        "\n",
        "# Nodes\n",
        "graph_builder.add_node(...\n",
        "graph_builder.add_node(...\n",
        "graph_builder.add_node(...\n",
        "graph_builder.add_node(...\n",
        "\n",
        "# Chatbot -> {ordering, tools, human, END}\n",
        "graph_builder.add_conditional_edges(\"chatbot\", maybe_route_to_tools)\n",
        "# Human -> {chatbot, END}\n",
        "graph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)\n",
        "\n",
        "# Tools (both kinds) always route back to chat afterwards.\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(\"ordering\", \"chatbot\")\n",
        "\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_with_order_tools = ...\n",
        "\n",
        "Image(graph_with_order_tools.get_graph().draw_mermaid_png())\n",
        "\n",
        "\n",
        "Now run the complete ordering system graph.\n",
        "\n",
        "\n",
        "\n",
        "# The default recursion limit for traversing nodes is 25 - setting it higher\n",
        "# means you can try a more complex order with multiple steps and round-trips.\n",
        "config = {\"recursion_limit\": 100}\n",
        "\n",
        "state = graph_with_order_tools.invoke({\"messages\": []}, config)\n",
        "\n",
        "# Things to try:\n",
        "# - Order a drink!\n",
        "# - Make a change to your order.\n",
        "# - \"Which teas are from England?\"\n",
        "# - Note that the graph should naturally exit after placing an order.\n",
        "\n",
        "pprint(state)\n",
        "\n",
        "\n",
        "Duration & Difficulty\n",
        "Duration (approx)\tDifficulty\n",
        "2 hours\t‚≠ê‚≠ê"
      ],
      "metadata": {
        "id": "puTBZc9HIvPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#‚úÖ √âTAPE 1 : Installation des biblioth√®ques"
      ],
      "metadata": {
        "id": "bSW9OrZMKy9i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "EfM9Yf2LIin2"
      },
      "outputs": [],
      "source": [
        "# Cette commande installe LangGraph (version sp√©cifique) et l'int√©gration LangChain pour Gemini API\n",
        "%pip install -qU 'langgraph==0.2.45' 'langchain-google-genai==2.0.4'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#‚úÖ √âTAPE 2 : Configuration de la cl√© API:\n",
        "**Explication :**\n",
        "Cette √©tape s√©curise l‚Äôacc√®s √† l‚ÄôAPI Gemini. LangChain utilise automatiquement cette variable d‚Äôenvironnement (GOOGLE_API_KEY) pour authentifier les requ√™tes."
      ],
      "metadata": {
        "id": "klI6N81FLHDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "# R√©cup√®re la cl√© API depuis les secrets de Kaggle\n",
        "#GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# D√©finit cette cl√© dans les variables d‚Äôenvironnement pour que l‚ÄôAPI Gemini puisse l‚Äôutiliser\n",
        "os.environ[\"GOOGLE_API_KEY\"] =\"xxxxxxxxxxxxxxxxxxxx\"\n"
      ],
      "metadata": {
        "id": "o0dkSBLkLV1D"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úÖ V√©rifie que tout fonctionne :"
      ],
      "metadata": {
        "id": "pdZMFXPZQcm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langgraph.graph import StateGraph\n",
        "from pprint import pprint\n",
        "\n",
        "# Mod√®le Gemini (rapide)\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n",
        "\n",
        "# Instructions syst√®me\n",
        "BARISTABOT_SYSINT = (\n",
        "    \"system\",\n",
        "    \"You are a BaristaBot, an interactive cafe ordering system. ...\"\n",
        ")\n",
        "WELCOME_MSG = \"Welcome to the BaristaBot cafe. Type `q` to quit. How may I serve you today?\"\n",
        "\n",
        "# Fonction chatbot simple\n",
        "def chatbot(state):\n",
        "    messages = [BARISTABOT_SYSINT] + state[\"messages\"]\n",
        "    return {\"messages\": [llm.invoke(messages)]}\n",
        "\n",
        "# Construction du graphe\n",
        "graph_builder = StateGraph(dict)\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "chat_graph = graph_builder.compile()\n",
        "\n",
        "# Test d'un tour de conversation\n",
        "user_msg = HumanMessage(content=\"What hot drinks do you have?\")\n",
        "state = chat_graph.invoke({\"messages\": [user_msg], \"order\": [], \"finished\": False})\n",
        "\n",
        "# Affichage du r√©sultat\n",
        "for msg in state[\"messages\"]:\n",
        "    print(f\"{type(msg).__name__}: {msg.content}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdvinmLfQgEB",
        "outputId": "1b0ddeb5-b646-4f5f-9538-dc0728fcf161"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AIMessage: We have a wide variety of hot drinks!  To help me narrow it down for you, what kind of flavour profile are you looking for?  Do you prefer something:\n",
            "\n",
            "* **Coffee based?** (e.g., Espresso, Americano, Latte, Cappuccino, Mocha, Flat White)  We also have decaf options!\n",
            "* **Tea based?** (e.g., Black tea, Green tea, Herbal tea - we have a selection of flavours)\n",
            "* **Something else entirely?** (e.g., Hot chocolate, Chai latte)\n",
            "\n",
            "\n",
            "Let me know your preference and I can give you a more detailed menu!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üß≠ √âtape suivante : √âtape 5 ‚Äî Visualiser le graphe"
      ],
      "metadata": {
        "id": "CU5eDLvPT7OA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "# Visualisation du graphe actuel\n",
        "Image(chat_graph.get_graph().draw_mermaid_png())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "CL_au7HgT_ep",
        "outputId": "f19541df-e327-44fb-86cc-d14b1e7f2b7a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "√âtape 6 ‚Äî Nouvelle interaction (second tour de conversation)\n",
        "Tu peux tester une deuxi√®me interaction avec l‚Äô√©tat actuel en ajoutant un nouveau message utilisateur :"
      ],
      "metadata": {
        "id": "kuqlDk33UKkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# Nouveau message utilisateur\n",
        "user_msg2 = HumanMessage(content=\"I think I‚Äôd like something tea-based with milk.\")\n",
        "\n",
        "# On ajoute ce message √† l'historique\n",
        "state[\"messages\"].append(user_msg2)\n",
        "\n",
        "# Et on relance le graphe\n",
        "state = chat_graph.invoke(state)\n",
        "\n",
        "# Affichage des messages\n",
        "for msg in state[\"messages\"]:\n",
        "    print(f\"{type(msg).__name__}: {msg.content}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kam4Uy5fUOKy",
        "outputId": "df739a22-e797-46ce-a8b4-9e78f7953471"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AIMessage: Excellent choice!  For tea-based drinks with milk, we offer:\n",
            "\n",
            "* **Chai Latte:** A spiced black tea blend with milk and a touch of sweetness.  We can adjust the sweetness level to your preference.\n",
            "* **Masala Chai Latte:** Similar to the Chai Latte, but with a more robust and complex blend of spices.\n",
            "* **Black Tea Latte:** Your choice of black tea (English Breakfast, Earl Grey, etc.) steamed with milk.  We can also add sweetener if desired.\n",
            "* **Green Tea Latte:** A lighter, more subtly flavored option with steamed milk.  Sweetener available upon request.\n",
            "\n",
            "\n",
            "Would you like to hear more about any of these options, or do you have a preference already?  We also offer different types of milk (dairy, almond, soy, oat).  Which milk would you prefer?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpr√©tation :\n",
        "Parfait üôå ! Le chatbot joue pleinement son r√¥le, en :\n",
        "\n",
        "respectant la logique de commande (il te propose des options concr√®tes de latte √† base de th√©),\n",
        "\n",
        "t‚Äôinterrogeant sur les pr√©f√©rences (sucrant, taille, vari√©t√©),\n",
        "\n",
        "adoptant un ton professionnel et engageant, comme demand√© dans l‚Äôinstruction syst√®me.\n",
        "\n",
        "Mais pour l‚Äôinstant :\n",
        "\n",
        "ü§ñ Le bot hallucine toujours le menu car il n‚Äôa pas encore acc√®s au vrai menu via un outil."
      ],
      "metadata": {
        "id": "14mOjBvuUe76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üß≠ √âtape suivante : √âtape 8 ‚Äì Ajouter un n≈ìud \"humain\" + boucle de conversation"
      ],
      "metadata": {
        "id": "eO_Ki7rMUwpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class OrderState(TypedDict):\n",
        "    \"\"\"√âtat de la commande pour BaristaBot.\"\"\"\n",
        "    messages: Annotated[list, add_messages]\n",
        "    order: list[str]\n",
        "    finished: bool\n"
      ],
      "metadata": {
        "id": "OIA7FlvAVwuW"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#‚úÖ Code √©tape 8 ‚Äì human_node + chatbot_with_welcome_msg\n",
        "\n",
        "from langchain_core.messages.ai import AIMessage\n",
        "\n",
        "def human_node(state: OrderState) -> OrderState:\n",
        "    \"\"\"Affiche le message de l‚ÄôAI et r√©cup√®re l‚Äôentr√©e utilisateur.\"\"\"\n",
        "    last_msg = state[\"messages\"][-1]\n",
        "    print(\"Model:\", last_msg.content)\n",
        "\n",
        "    user_input = input(\"User: \")\n",
        "\n",
        "    # Si l‚Äôutilisateur veut quitter, on met `finished = True`\n",
        "    if user_input.lower() in {\"q\", \"quit\", \"exit\", \"goodbye\"}:\n",
        "        state[\"finished\"] = True\n",
        "\n",
        "    # On retourne l‚Äô√©tat avec le nouveau message utilisateur ajout√©\n",
        "    return state | {\"messages\": [(\"user\", user_input)]}\n",
        "\n",
        "def chatbot_with_welcome_msg(state: OrderState) -> OrderState:\n",
        "    \"\"\"Chatbot qui affiche un message de bienvenue au d√©marrage.\"\"\"\n",
        "    if state[\"messages\"]:\n",
        "        # Conversation d√©j√† entam√©e\n",
        "        output = llm.invoke([BARISTABOT_SYSINT] + state[\"messages\"])\n",
        "    else:\n",
        "        # Premier message ‚Üí on commence avec le message de bienvenue\n",
        "        output = AIMessage(content=WELCOME_MSG)\n",
        "\n",
        "    return state | {\"messages\": [output]}\n"
      ],
      "metadata": {
        "id": "Lf9-JUSeU5Bb"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from typing import Literal\n",
        "\n",
        "# üß† Fonction conditionnelle : si l'utilisateur tape 'q' ou 'quit', on quitte le graphe\n",
        "def maybe_exit_human_node(state: OrderState) -> Literal[\"chatbot\", \"__end__\"]:\n",
        "    if state.get(\"finished\", False):\n",
        "        return END\n",
        "    else:\n",
        "        return \"chatbot\"\n",
        "\n",
        "# üìà Construction du graphe LangGraph\n",
        "graph_builder = StateGraph(OrderState)\n",
        "\n",
        "# Ajout des n≈ìuds\n",
        "graph_builder.add_node(\"chatbot\", chatbot_with_welcome_msg)\n",
        "graph_builder.add_node(\"human\", human_node)\n",
        "\n",
        "# Transitions\n",
        "graph_builder.add_edge(\"chatbot\", \"human\")  # Apr√®s chaque r√©ponse, on va vers humain\n",
        "graph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)  # Si pas fini ‚Üí chatbot\n",
        "\n",
        "# Point d'entr√©e\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "\n",
        "# Compilation du graphe\n",
        "chat_with_human_graph = graph_builder.compile()\n"
      ],
      "metadata": {
        "id": "Q7vpve9cYLOu"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Construction du graphe avec boucle de dialogue"
      ],
      "metadata": {
        "id": "TgtkDo4FVJON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph_builder = StateGraph(OrderState)\n",
        "\n",
        "# Ajout des n≈ìuds\n",
        "graph_builder.add_node(\"chatbot\", chatbot_with_welcome_msg)\n",
        "graph_builder.add_node(\"human\", human_node)\n",
        "\n",
        "# Transition : chatbot ‚Üí human\n",
        "graph_builder.add_edge(\"chatbot\", \"human\")\n",
        "\n",
        "# Condition : human ‚Üí chatbot OU END\n",
        "from typing import Literal\n",
        "from langgraph.graph import END\n",
        "\n",
        "def maybe_exit_human_node(state: OrderState) -> Literal[\"chatbot\", \"__end__\"]:\n",
        "    if state.get(\"finished\", False):\n",
        "        return END  # Quitte le graphe\n",
        "    else:\n",
        "        return \"chatbot\"\n",
        "\n",
        "graph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)\n",
        "\n",
        "# Point d‚Äôentr√©e\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "\n",
        "# Compilation\n",
        "chat_with_human_graph = graph_builder.compile()\n"
      ],
      "metadata": {
        "id": "k2a9tYszVLlc"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#‚úÖ Lancer la conversation compl√®te\n",
        "# L'√©tat initial vide (d√©but de conversation)\n",
        "initial_state = {\n",
        "    \"messages\": [],\n",
        "    \"order\": [],\n",
        "    \"finished\": False\n",
        "}\n",
        "\n",
        "# Lancement : chatbot ‚Üí human ‚Üí boucle jusqu‚Äô√† finished = True\n",
        "chat_with_human_graph.invoke(initial_state)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEOdefY9YbF1",
        "outputId": "a39e77c6-7595-455d-843b-4f80763ad5d5"
      },
      "execution_count": 42,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: Welcome to the BaristaBot cafe. Type `q` to quit. How may I serve you today?\n",
            "User: TEA\n",
            "Model: Great choice! What kind of tea would you like?  We have Black Tea, Green Tea, White Tea, Herbal Tea, and Chai.\n",
            "\n",
            "User: Q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [AIMessage(content='Welcome to the BaristaBot cafe. Type `q` to quit. How may I serve you today?', additional_kwargs={}, response_metadata={}, id='0e5dab7f-bdb8-403b-9fbd-b1fd4cb22aec'),\n",
              "  HumanMessage(content='TEA', additional_kwargs={}, response_metadata={}, id='eef2bf47-ad1b-4955-b1c8-3d9ba0cf0147'),\n",
              "  AIMessage(content='Great choice! What kind of tea would you like?  We have Black Tea, Green Tea, White Tea, Herbal Tea, and Chai.\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--e90d5c44-a68d-416f-ba56-2eb38e19afbf-0', usage_metadata={'input_tokens': 37, 'output_tokens': 30, 'total_tokens': 67, 'input_token_details': {'cache_read': 0}}),\n",
              "  HumanMessage(content='Q', additional_kwargs={}, response_metadata={}, id='15b9ba41-24ea-434a-9bcc-9a92807b3306')],\n",
              " 'order': [],\n",
              " 'finished': True}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##√©tape 10 : ajout du menu dynamique avec un outil LangGraph\n",
        "\n",
        "üß† Objectif de cette √©tape :\n",
        "Cr√©er une fonction Python annot√©e avec @tool ‚Üí get_menu().\n",
        "\n",
        "L‚Äôint√©grer dans le graphe via un ToolNode.\n",
        "\n",
        "L‚Äôagent pourra appeler cet outil de mani√®re automatique (ex : si tu dis \"What teas do you have?\", il appellera get_menu())."
      ],
      "metadata": {
        "id": "ImF4CGB3Y9bA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#‚úÖ √âtape 10 ‚Äî\n",
        "#1. ‚úçÔ∏è D√©finir le menu avec @tool\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def get_menu() -> str:\n",
        "    \"\"\"Provide the latest up-to-date menu.\"\"\"\n",
        "    return \"\"\"\n",
        "MENU:\n",
        "\n",
        "‚òï Coffee Drinks:\n",
        "  - Espresso\n",
        "  - Americano\n",
        "  - Cold Brew\n",
        "\n",
        "‚òï Coffee Drinks with Milk:\n",
        "  - Latte\n",
        "  - Cappuccino\n",
        "  - Cortado\n",
        "  - Macchiato\n",
        "  - Mocha\n",
        "  - Flat White\n",
        "\n",
        "üçµ Tea Drinks:\n",
        "  - English Breakfast Tea\n",
        "  - Green Tea\n",
        "  - Earl Grey\n",
        "\n",
        "üçµ Tea Drinks with Milk:\n",
        "  - Chai Latte\n",
        "  - Matcha Latte\n",
        "  - London Fog\n",
        "\n",
        "üç´ Other Drinks:\n",
        "  - Steamer\n",
        "  - Hot Chocolate\n",
        "\n",
        "üß© Modifiers:\n",
        "  - Milk: Whole, 2%, Oat, Almond, 2% Lactose Free (default: Whole)\n",
        "  - Espresso shots: Single, Double, Triple, Quadruple (default: Double)\n",
        "  - Caffeine: Regular, Decaf (default: Regular)\n",
        "  - Temperature: Hot, Iced (default: Hot)\n",
        "  - Sweeteners: Vanilla, Hazelnut, Caramel, Chocolate, Sugar-free Vanilla\n",
        "  - Special requests: \"extra hot\", \"half caff\", \"extra foam\", etc.\n",
        "\n",
        "üö´ Soy milk is out of stock today.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "cFBW62rtYyoo"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. ü§ñ Attacher ce menu √† l‚Äôagent via un ToolNode\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# D√©claration des outils automatiques (ici juste le menu)\n",
        "tools = [get_menu]\n",
        "\n",
        "# N≈ìud outil pour LangGraph\n",
        "tool_node = ToolNode(tools)\n",
        "\n",
        "# Mod√®le Gemini avec les outils li√©s\n",
        "llm_with_tools = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\").bind_tools(tools)\n"
      ],
      "metadata": {
        "id": "IkaQ4i0TZZTR"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3. üîÅ Mettre √† jour le chatbot pour qu‚Äôil utilise llm_with_tools\n",
        "\n",
        "from langchain_core.messages.ai import AIMessage\n",
        "\n",
        "def chatbot_with_tools(state: OrderState) -> OrderState:\n",
        "    \"\"\"Chatbot qui conna√Æt les outils (comme get_menu).\"\"\"\n",
        "    if state[\"messages\"]:\n",
        "        output = llm_with_tools.invoke([BARISTABOT_SYSINT] + state[\"messages\"])\n",
        "    else:\n",
        "        output = AIMessage(content=WELCOME_MSG)\n",
        "\n",
        "    return state | {\"messages\": [output]}\n"
      ],
      "metadata": {
        "id": "WqwKI5MgZgyY"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. üîÄ Ajouter une condition de transition chatbot ‚Üí tools ou human\n",
        "def maybe_route_to_tools(state: OrderState) -> Literal[\"tools\", \"human\"]:\n",
        "    if not state.get(\"messages\"):\n",
        "        raise ValueError(\"No messages found in state.\")\n",
        "    msg = state[\"messages\"][-1]\n",
        "\n",
        "    # Si le chatbot appelle un outil ‚Üí aller au n≈ìud 'tools'\n",
        "    if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
        "        return \"tools\"\n",
        "    else:\n",
        "        return \"human\"\n"
      ],
      "metadata": {
        "id": "MwX5nnozZqAJ"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. üîß Reconstruire un graphe complet avec outils\n",
        "graph_builder = StateGraph(OrderState)\n",
        "\n",
        "# Ajouter les n≈ìuds\n",
        "graph_builder.add_node(\"chatbot\", chatbot_with_tools)\n",
        "graph_builder.add_node(\"human\", human_node)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "# Transitions\n",
        "graph_builder.add_conditional_edges(\"chatbot\", maybe_route_to_tools)\n",
        "graph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "\n",
        "# Point d‚Äôentr√©e\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "\n",
        "# Compiler\n",
        "graph_with_menu = graph_builder.compile()\n"
      ],
      "metadata": {
        "id": "ItUjKIkHZxZg"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6. üöÄ Tester le nouveau graphe\n",
        "state = {\n",
        "    \"messages\": [],\n",
        "    \"order\": [],\n",
        "    \"finished\": False\n",
        "}\n",
        "\n",
        "# Lancer la conversation avec menu disponible\n",
        "graph_with_menu.invoke(state)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GrjjbLUZ5FJ",
        "outputId": "8ca765a5-b07f-4d90-a233-b5b4d05bf1af"
      },
      "execution_count": 48,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: Welcome to the BaristaBot cafe. Type `q` to quit. How may I serve you today?\n",
            "User: Q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [AIMessage(content='Welcome to the BaristaBot cafe. Type `q` to quit. How may I serve you today?', additional_kwargs={}, response_metadata={}, id='19a8cfa9-0c31-4ca2-aa31-e8fd57b0cf3c'),\n",
              "  HumanMessage(content='Q', additional_kwargs={}, response_metadata={}, id='1b0bb154-f425-48b1-8bcf-f245ab5e6db5')],\n",
              " 'order': [],\n",
              " 'finished': True}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ##üß∞ √âtape 11 ‚Äî D√©claration des outils de commande\n",
        " üß† Objectif de cette √©tape\n",
        "permettre au bot de :\n",
        "\n",
        "1. Ajouter une boisson √† la commande.\n",
        "\n",
        "2. Voir la commande en cours.\n",
        "\n",
        "3. R√©initialiser la commande.\n",
        "\n",
        "4. Confirmer avec l'utilisateur.\n",
        "\n",
        "5. Passer la commande et terminer.\n",
        "\n"
      ],
      "metadata": {
        "id": "Kn_xFy36aSxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#l‚Äô√©tape 11 : G√©rer les commandes dans BaristaBot.\n",
        "#‚úÖ 1. D√©finir les outils avec @tool\n",
        "from langchain_core.tools import tool\n",
        "from collections.abc import Iterable\n",
        "\n",
        "@tool\n",
        "def add_to_order(drink: str, modifiers: Iterable[str]) -> str:\n",
        "    \"\"\"Adds the specified drink to the customer's order, including any modifiers.\"\"\"\n",
        "    pass\n",
        "\n",
        "@tool\n",
        "def confirm_order() -> str:\n",
        "    \"\"\"Asks the customer if the order is correct.\"\"\"\n",
        "    pass\n",
        "\n",
        "@tool\n",
        "def get_order() -> str:\n",
        "    \"\"\"Returns the user's order so far.\"\"\"\n",
        "    pass\n",
        "\n",
        "@tool\n",
        "def clear_order():\n",
        "    \"\"\"Clears the entire order.\"\"\"\n",
        "    pass\n",
        "\n",
        "@tool\n",
        "def place_order() -> int:\n",
        "    \"\"\"Finalizes the order and returns ETA in minutes.\"\"\"\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "dYUUs1vAa6nD"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#‚úÖ 2. Impl√©menter le order_node\n",
        "from langgraph.prebuilt import InjectedState\n",
        "from langchain_core.messages.tool import ToolMessage\n",
        "from random import randint\n",
        "\n",
        "def order_node(state: OrderState) -> OrderState:\n",
        "    \"\"\"N≈ìud qui g√®re l'√©tat de la commande (add, clear, confirm, place).\"\"\"\n",
        "    tool_msg = state[\"messages\"][-1]\n",
        "    order = state.get(\"order\", [])\n",
        "    outbound_msgs = []\n",
        "    order_placed = False\n",
        "\n",
        "    for tool_call in tool_msg.tool_calls:\n",
        "\n",
        "        if tool_call[\"name\"] == \"add_to_order\":\n",
        "            drink = tool_call[\"args\"][\"drink\"]\n",
        "            modifiers = tool_call[\"args\"].get(\"modifiers\", [])\n",
        "            modifier_str = \", \".join(modifiers) if modifiers else \"no modifiers\"\n",
        "            order.append(f\"{drink} ({modifier_str})\")\n",
        "            response = \"\\n\".join(order)\n",
        "\n",
        "        elif tool_call[\"name\"] == \"get_order\":\n",
        "            response = \"\\n\".join(order) if order else \"(no order)\"\n",
        "\n",
        "        elif tool_call[\"name\"] == \"clear_order\":\n",
        "            order.clear()\n",
        "            response = \"Order cleared.\"\n",
        "\n",
        "        elif tool_call[\"name\"] == \"confirm_order\":\n",
        "            print(\"Your order:\")\n",
        "            for item in order:\n",
        "                print(f\"  - {item}\")\n",
        "            response = input(\"Is this correct? \")\n",
        "\n",
        "        elif tool_call[\"name\"] == \"place_order\":\n",
        "            print(\"‚úÖ Sending order to kitchen:\")\n",
        "            for item in order:\n",
        "                print(f\"  - {item}\")\n",
        "            order_placed = True\n",
        "            response = randint(3, 7)  # Random ETA in minutes\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Unknown tool: {tool_call['name']}\")\n",
        "\n",
        "        outbound_msgs.append(\n",
        "            ToolMessage(\n",
        "                content=response,\n",
        "                name=tool_call[\"name\"],\n",
        "                tool_call_id=tool_call[\"id\"],\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        \"messages\": outbound_msgs,\n",
        "        \"order\": order,\n",
        "        \"finished\": order_placed\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Uk1AmEZkadlK"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#‚úÖ 3. Ajouter le routing : chatbot ‚Üí tools ou ordering\n",
        "def maybe_route_to_tools(state: OrderState) -> str:\n",
        "    if state.get(\"finished\", False):\n",
        "        return \"__end__\"\n",
        "\n",
        "    msg = state[\"messages\"][-1]\n",
        "\n",
        "    if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
        "        tool_names = [tool[\"name\"] for tool in msg.tool_calls]\n",
        "        if any(tool in [\"get_menu\"] for tool in tool_names):\n",
        "            return \"tools\"\n",
        "        else:\n",
        "            return \"ordering\"\n",
        "\n",
        "    return \"human\"\n"
      ],
      "metadata": {
        "id": "qTvlC_mSbO1V"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#‚úÖ 4. D√©finir tous les outils et le graphe final\n",
        "# üîß Outils automatiques\n",
        "auto_tools = [get_menu]\n",
        "tool_node = ToolNode(auto_tools)\n",
        "\n",
        "# üîß Outils li√©s √† l‚Äô√©tat (commande)\n",
        "order_tools = [add_to_order, get_order, clear_order, confirm_order, place_order]\n",
        "\n",
        "# ü§ñ Mod√®le complet avec tous les outils\n",
        "llm_with_tools = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\").bind_tools(auto_tools + order_tools)\n",
        "\n",
        "# ‚ôªÔ∏è Mise √† jour du chatbot\n",
        "def chatbot_with_order_tools(state: OrderState) -> OrderState:\n",
        "    if state[\"messages\"]:\n",
        "        output = llm_with_tools.invoke([BARISTABOT_SYSINT] + state[\"messages\"])\n",
        "    else:\n",
        "        output = AIMessage(content=WELCOME_MSG)\n",
        "    return state | {\"messages\": [output]}\n"
      ],
      "metadata": {
        "id": "CEceLpFzbVPN"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#‚úÖ 5. Construire le graphe final complet\n",
        "graph_builder = StateGraph(OrderState)\n",
        "\n",
        "# N≈ìuds\n",
        "graph_builder.add_node(\"chatbot\", chatbot_with_order_tools)\n",
        "graph_builder.add_node(\"human\", human_node)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "graph_builder.add_node(\"ordering\", order_node)\n",
        "\n",
        "# Transitions\n",
        "graph_builder.add_conditional_edges(\"chatbot\", maybe_route_to_tools)\n",
        "graph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(\"ordering\", \"chatbot\")\n",
        "\n",
        "# Entr√©e\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "\n",
        "# Compile\n",
        "graph_with_order_tools = graph_builder.compile()\n"
      ],
      "metadata": {
        "id": "yGG1mpWgbiVc"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#‚úÖ 6. Lancer la version compl√®te de BaristaBot\n",
        "state = {\n",
        "    \"messages\": [],\n",
        "    \"order\": [],\n",
        "    \"finished\": False\n",
        "}\n",
        "\n",
        "graph_with_order_tools.invoke(state)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pAtHJEHbtOs",
        "outputId": "4d87e3f6-7772-4baf-e4c6-dac0216c5d87"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: Welcome to the BaristaBot cafe. Type `q` to quit. How may I serve you today?\n",
            "User: I‚Äôd like an Iced Matcha Latte with oat milk and one pump of vanilla, please.\n",
            "Model: Great, I have added an Iced Matcha Latte with oat milk and one pump of vanilla to your order.  Anything else?\n",
            "\n",
            "User: NO\n",
            "Model: Okay, your order is one Iced Matcha Latte with oat milk and one pump of vanilla.  Is that correct?\n",
            "\n",
            "User: YES\n",
            "Your order:\n",
            "  - Iced Matcha Latte (oat milk, one pump of vanilla)\n",
            "Is this correct? yes\n",
            "‚úÖ Sending order to kitchen:\n",
            "  - Iced Matcha Latte (oat milk, one pump of vanilla)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [AIMessage(content='Welcome to the BaristaBot cafe. Type `q` to quit. How may I serve you today?', additional_kwargs={}, response_metadata={}, id='b7d1ef64-136c-4161-a287-a00558b6c54a'),\n",
              "  HumanMessage(content='I‚Äôd like an Iced Matcha Latte with oat milk and one pump of vanilla, please.', additional_kwargs={}, response_metadata={}, id='bf03a995-71f2-46a4-a79e-2b9b92b6f423'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'add_to_order', 'arguments': '{\"modifiers\": [\"oat milk\", \"one pump of vanilla\"], \"drink\": \"Iced Matcha Latte\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--a8d7b5db-1b1a-4030-86ac-d983e13ddbf7-0', tool_calls=[{'name': 'add_to_order', 'args': {'modifiers': ['oat milk', 'one pump of vanilla'], 'drink': 'Iced Matcha Latte'}, 'id': '77b84708-66a7-4539-a809-a095a3affeb1', 'type': 'tool_call'}], usage_metadata={'input_tokens': 143, 'output_tokens': 17, 'total_tokens': 160, 'input_token_details': {'cache_read': 0}}),\n",
              "  ToolMessage(content='Iced Matcha Latte (oat milk, one pump of vanilla)', name='add_to_order', id='82600df1-85ee-43c8-8f85-5ba48d227d88', tool_call_id='77b84708-66a7-4539-a809-a095a3affeb1'),\n",
              "  AIMessage(content='Great, I have added an Iced Matcha Latte with oat milk and one pump of vanilla to your order.  Anything else?\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--7e4d03a8-0064-428a-8eb8-03d9099621c4-0', usage_metadata={'input_tokens': 179, 'output_tokens': 26, 'total_tokens': 205, 'input_token_details': {'cache_read': 0}}),\n",
              "  HumanMessage(content='NO', additional_kwargs={}, response_metadata={}, id='194fe996-dbd8-4aec-9d69-84e6fe90aff0'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_order', 'arguments': '{}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--313b6a4d-d2cc-4bcf-90e4-71cc18678161-0', tool_calls=[{'name': 'get_order', 'args': {}, 'id': 'fb0d25d3-89d9-42fb-b31a-d8490b1a4686', 'type': 'tool_call'}], usage_metadata={'input_tokens': 206, 'output_tokens': 3, 'total_tokens': 209, 'input_token_details': {'cache_read': 0}}),\n",
              "  ToolMessage(content='Iced Matcha Latte (oat milk, one pump of vanilla)', name='get_order', id='14946655-d6c1-4bf1-bcee-720959fd02d7', tool_call_id='fb0d25d3-89d9-42fb-b31a-d8490b1a4686'),\n",
              "  AIMessage(content='Okay, your order is one Iced Matcha Latte with oat milk and one pump of vanilla.  Is that correct?\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--4adea387-fd94-4754-bb68-03578842e807-0', usage_metadata={'input_tokens': 226, 'output_tokens': 24, 'total_tokens': 250, 'input_token_details': {'cache_read': 0}}),\n",
              "  HumanMessage(content='YES', additional_kwargs={}, response_metadata={}, id='800cd734-9c1a-4b2c-8d5c-3b13554bd6fa'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'confirm_order', 'arguments': '{}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--080085d0-fd54-4c84-b590-627613d5d30b-0', tool_calls=[{'name': 'confirm_order', 'args': {}, 'id': '62a3eaf8-0792-42f2-ab68-319c94217746', 'type': 'tool_call'}], usage_metadata={'input_tokens': 251, 'output_tokens': 3, 'total_tokens': 254, 'input_token_details': {'cache_read': 0}}),\n",
              "  ToolMessage(content='yes', name='confirm_order', id='256644b8-b038-40c3-bbbd-9c5c1b40e438', tool_call_id='62a3eaf8-0792-42f2-ab68-319c94217746'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'place_order', 'arguments': '{}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--470958e6-2509-45fd-91bd-d0187a858b8d-0', tool_calls=[{'name': 'place_order', 'args': {}, 'id': '905d51da-6fc2-43d8-9961-5da51b5b6054', 'type': 'tool_call'}], usage_metadata={'input_tokens': 259, 'output_tokens': 3, 'total_tokens': 262, 'input_token_details': {'cache_read': 0}}),\n",
              "  ToolMessage(content='7', name='place_order', id='6d86de7b-5891-43fa-864b-3b0da677706c', tool_call_id='905d51da-6fc2-43d8-9961-5da51b5b6054'),\n",
              "  AIMessage(content='Perfect! Your order will be ready in 7 minutes.  Enjoy!\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--946c0f40-1439-4b50-95a5-8ef7d784d880-0', usage_metadata={'input_tokens': 267, 'output_tokens': 16, 'total_tokens': 283, 'input_token_details': {'cache_read': 0}})],\n",
              " 'order': ['Iced Matcha Latte (oat milk, one pump of vanilla)'],\n",
              " 'finished': True}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    }
  ]
}