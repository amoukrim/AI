{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amoukrim/AI/blob/main/Week13/DailyChallenge/dailyChallengew_13_d2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c63bff58",
      "metadata": {
        "id": "c63bff58"
      },
      "source": [
        "#@Author : Adil MOUKRIM # Application Agentic RAG Streamlit\n",
        "Dernière mise à jour : 17 août 2025\n",
        "\n",
        "Défi quotidien : Application Agentic RAG Streamlit + Agent utilisant des outils\n",
        "\n",
        "\n",
        "Ce que vous devriez faire\n",
        "app.py: Échafaudage Streamlit qui charge les clés API depuis .env, définit les indicateurs de traçage LangSmith, fournit une zone de texte et un bouton « Envoyer »\n",
        "et renvoie une réponse simulée . Il tente également de charger agentic_rag.ipynbsous forme de texte.\n",
        "agentic_rag.ipynb: votre bloc-notes où résidera l'agent réel / le pipeline RAG.\n",
        "Les clés lues dansapp.py : GOOGLE_API_KEY, TAVILY_API_KEY, GROQ_API_KEY, LANGCHAIN_API_KEYet les variables LangSmith sont définies\n",
        "( LANGCHAIN_TRACING_V2, LANGCHAIN_ENDPOINT). Cela suggère que votre agent doit être construit avec LangChain , la recherche Tavily , Groq LLM et, éventuellement, les outils Google.\n",
        "\n",
        "\n",
        "\n",
        "Objectifs d'apprentissage\n",
        "Créez un récupérateur (index vectoriel) et un agent qui peuvent appeler des outils (par exemple, une recherche sur le Web) et fonder les réponses sur le contenu récupéré.\n",
        "Orchestrer une boucle de raisonnement → récupérer → lire → synthétiser avec attribution de source.\n",
        "Exposez une API Python propre que l'application Streamlit peut appeler.\n",
        "Gérez les erreurs/délais d'attente avec élégance ; connectez-vous à LangSmith lorsque cette option est activée.\n",
        "\n",
        "\n",
        "Environnement requis\n",
        "Un .envfichier contenant au moins :\n",
        "GROQ_API_KEY,TAVILY_API_KEY\n",
        "(Facultatif) GOOGLE_API_KEY, LANGCHAIN_API_KEYpour des outils supplémentaires et le traçage\n",
        "Paquets Python (suggérés) : streamlit, langchain, langchain-community, langchain-groq, tavily-python, faiss-cpu, tiktoken,python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bcdb1e5",
      "metadata": {
        "id": "5bcdb1e5"
      },
      "source": [
        "Analyse complète et validée du projet **Application Agentic RAG Streamlit** :\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Structure du projet\n",
        "\n",
        "* **`app.py`** :\n",
        "\n",
        "  * Sert d’interface utilisateur via **Streamlit**.\n",
        "  * Charge les clés API depuis `.env` grâce à `python-dotenv`.\n",
        "  * Définit les indicateurs de traçage **LangSmith** (facultatif mais cohérent si `LANGCHAIN_TRACING_V2` et `LANGCHAIN_ENDPOINT` sont renseignés).\n",
        "  * Zone de texte + bouton « Envoyer » → appel à l’agent.\n",
        "  * Simule une réponse + possibilité d’afficher le contenu de `agentic_rag.ipynb` sous forme de texte.\n",
        "\n",
        "* **`agentic_rag.ipynb`** :\n",
        "\n",
        "  * Contient le pipeline réel : **RAG (retrieval-augmented generation)** + agent capable d’appeler des outils.\n",
        "  * Les clés API de **Groq**, **Tavily**, éventuellement **Google** et **LangChainHub** sont prévues pour activer des outils.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Objectifs pédagogiques\n",
        "\n",
        "* Construire un **retriever** (index vectoriel : FAISS ou ChromaDB).\n",
        "* Orchestrer un agent LangChain qui fait :\n",
        "\n",
        "  1. **raisonnement** →\n",
        "  2. **recherche contextuelle (Tavily / Chroma / FAISS)** →\n",
        "  3. **lecture / synthèse avec attribution des sources**.\n",
        "* Exposer une API Python claire pour que `app.py` puisse appeler le pipeline.\n",
        "* Gérer **timeouts** et **exceptions** de façon robuste.\n",
        "* Connecter les logs à **LangSmith** (si activé).\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Variables d’environnement\n",
        "\n",
        "Doivent figurer dans `.env` :\n",
        "\n",
        "* **Obligatoires** :\n",
        "\n",
        "  * `GROQ_API_KEY` (accès au LLM Groq)\n",
        "  * `TAVILY_API_KEY` (moteur de recherche Tavily)\n",
        "* **Optionnelles mais prévues** :\n",
        "\n",
        "  * `GOOGLE_API_KEY` (Google Generative AI, recherche ou Palm2/Gemini)\n",
        "  * `LANGCHAIN_API_KEY` (traçage avec LangSmith)\n",
        "  * `LANGCHAIN_TRACING_V2`, `LANGCHAIN_ENDPOINT`\n",
        "\n",
        "Validation : cohérent avec la stack **LangChain + RAG + outils externes**.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Paquets Python nécessaires\n",
        "\n",
        "### Mentionnés dans la description\n",
        "\n",
        "* `streamlit` : interface web.\n",
        "* `langchain`, `langchain-community` : cœur LangChain + outils communautaires.\n",
        "* `langchain-groq` : wrapper pour LLM Groq.\n",
        "* `tavily-python` : client API Tavily.\n",
        "* `faiss-cpu` : index vectoriel performant.\n",
        "* `tiktoken` : tokenisation (OpenAI & compatibilité Groq).\n",
        "* `python-dotenv` : lecture du `.env`.\n",
        "\n",
        "### Dans requirements.txt\n",
        "\n",
        "* `langgraph` : utile pour orchestrer des graphes d’agents (workflow structuré).\n",
        "* `langchainhub` : récupérer des prompts/outils pré-packagés.\n",
        "* `ipykernel` : exécution du notebook.\n",
        "* `langchain_huggingface` : wrapper pour Hugging Face Hub.\n",
        "* `bs4` (BeautifulSoup) : parsing HTML pour nettoyage.\n",
        "* `chromadb` : alternative à FAISS (vector DB locale).\n",
        "* `langchain_google_genai` : intégration Google Generative AI.\n",
        "\n",
        "Validation :\n",
        "\n",
        "* **Pas de conflit majeur**. Tous ces packages sont compatibles avec Python 3.11+ (ton setup Win10/VSCode est OK).\n",
        "* Tu dois juste veiller à installer **faiss-cpu** via pip et pas conda pour éviter les soucis de build.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Compatibilité des outils\n",
        "\n",
        "* **Groq** : nécessite `langchain-groq`, clé API Groq → OK.\n",
        "* **Tavily** : nécessite `tavily-python`, clé API Tavily → OK.\n",
        "* **Google GenAI** : nécessite `langchain_google_genai`, clé Google → optionnel mais compatible.\n",
        "* **Vector store** : FAISS ou ChromaDB → tous deux installés, interchangeables.\n",
        "* **LangSmith** : supporté si `LANGCHAIN_API_KEY` et `langchain` ≥ 0.2.0.\n",
        "\n",
        "Tous les outils mentionnés sont disponibles, intégrables et compatibles dans un même pipeline **Agentic RAG**.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Environnement d’exécution\n",
        "\n",
        "### Étapes validées :\n",
        "\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "cp .env.example .env   # puis ajouter les clés\n",
        "streamlit run app.py\n",
        "```\n",
        "\n",
        "* Windows 10 + Python 3.11 : compatible.\n",
        "* VSCode + Jupyter/IPython pour travailler dans `agentic_rag.ipynb`.\n",
        "* Streamlit sert l’UI → accessible sur [http://localhost:8501](http://localhost:8501).\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Risques / points d’attention\n",
        "\n",
        "* **Groq API** : certains modèles ne gèrent pas encore toutes les fonctions avancées (tool calling, JSON mode). Vérifier la version `langchain-groq`.\n",
        "* **Tavily** : gratuit mais quotas → prévoir fallback si quota dépassé.\n",
        "* **Streamlit** : attention aux longs appels LLM → ajouter gestion de timeout (`asyncio.wait_for` ou `requests` avec `timeout`).\n",
        "* **LangSmith** : peut générer des erreurs si `LANGCHAIN_ENDPOINT` n’est pas configuré correctement.\n",
        "* **FAISS vs Chroma** : choisir un seul retriever pour éviter confusion.\n",
        "\n",
        "---\n",
        "\n",
        " **Validation finale** :\n",
        "\n",
        "* Les **outils, packages et variables d’environnement** listés sont cohérents et compatibles.\n",
        "* Le projet est fonctionnel sous **Win10 + Python 3.11 + VSCode + Streamlit**.\n",
        "* Aucun package critique manquant, mais je recommande d’ajouter explicitement :\n",
        "\n",
        "  * `python-dotenv` (si pas déjà dans requirements).\n",
        "  * `requests` (souvent nécessaire pour Tavily/Google).\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8b09d52",
      "metadata": {
        "id": "f8b09d52"
      },
      "source": [
        "Voici une **organisation optimale et claire du pipeline RAG Agentic** a utiliser dans le projet (`agentic_rag.ipynb`) et connecter à `app.py`.\n",
        "\n",
        "---\n",
        "\n",
        "# 1. Architecture globale\n",
        "\n",
        "```\n",
        "Utilisateur (Streamlit UI)\n",
        "        |\n",
        "        v\n",
        "   app.py (frontend)\n",
        "        |\n",
        "        v\n",
        "API interne (appel à une fonction Python)\n",
        "        |\n",
        "        v\n",
        "  Agent RAG (LangChain)\n",
        "        |\n",
        "   ┌───────────────┬────────────────┬─────────────────┐\n",
        "   |               |                |                 |\n",
        "Retriever (FAISS/Chroma)   Web Search (Tavily)   Google Tools (optionnel)\n",
        "   |               |                |\n",
        "   └───────────────┴────────────────┴─────────────→ Fusion du contexte\n",
        "        |\n",
        "        v\n",
        "    Synthèse avec attribution (Groq LLM)\n",
        "        |\n",
        "        v\n",
        "   Réponse finale → Streamlit\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# 2. Étapes du pipeline\n",
        "\n",
        "## a) Chargement des clés\n",
        "\n",
        "```python\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
        "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")  # optionnel\n",
        "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")  # optionnel\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## b) Initialisation du LLM (Groq)\n",
        "\n",
        "```python\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(\n",
        "    groq_api_key=GROQ_API_KEY,\n",
        "    model=\"mixtral-8x7b-32768\",  # modèle conseillé pour RAG\n",
        "    temperature=0.2,\n",
        "    max_tokens=2048\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## c) Index vectoriel (retriever)\n",
        "\n",
        "Option 1 : **FAISS**\n",
        "\n",
        "```python\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.load_local(\"faiss_index\", embeddings)\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
        "```\n",
        "\n",
        "Option 2 : **ChromaDB**\n",
        "\n",
        "```python\n",
        "from langchain_community.vectorstores import Chroma\n",
        "vectorstore = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## d) Outil de recherche externe (Tavily)\n",
        "\n",
        "```python\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "tavily_tool = TavilySearchResults(api_key=TAVILY_API_KEY, k=3)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## e) Agent RAG (avec tools)\n",
        "\n",
        "```python\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "\n",
        "tools = [tavily_tool]  # tu peux ajouter un tool Google si besoin\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent=AgentType.OPENAI_FUNCTIONS,  # Groq compatible avec JSON tool calling\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## f) Orchestration complète (raisonnement → retrieve → synthèse)\n",
        "\n",
        "```python\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    chain_type=\"stuff\",  # simple concaténation, peut être \"map_reduce\" si long\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "def agentic_rag_pipeline(query: str):\n",
        "    # Étape 1 : réponse basée sur vectorstore\n",
        "    result = qa_chain({\"query\": query})\n",
        "    \n",
        "    # Étape 2 : si la confiance est faible ou info manquante → web search\n",
        "    if \"je ne sais pas\" in result[\"result\"].lower():\n",
        "        web_info = agent.run(query)\n",
        "        return f\"{result['result']}\\n\\nInfos complémentaires (Web): {web_info}\"\n",
        "    \n",
        "    return result[\"result\"]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# 3. Intégration avec Streamlit (`app.py`)\n",
        "\n",
        "```python\n",
        "import streamlit as st\n",
        "from agentic_rag import agentic_rag_pipeline  # fonction ci-dessus\n",
        "\n",
        "st.title(\"Agentic RAG avec Groq + Tavily + FAISS\")\n",
        "query = st.text_area(\"Pose ta question :\")\n",
        "\n",
        "if st.button(\"Envoyer\"):\n",
        "    with st.spinner(\"L'agent réfléchit...\"):\n",
        "        try:\n",
        "            response = agentic_rag_pipeline(query)\n",
        "            st.write(response)\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur: {e}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# 4. Points clés de robustesse\n",
        "\n",
        "* **Timeouts** : ajouter `timeout=30` dans les appels LLM / Tavily.\n",
        "* **Attribution des sources** : extraire `result[\"source_documents\"]` pour afficher titres/URLs dans Streamlit.\n",
        "* **Fallback** : si Groq est down → prévoir HuggingFace ou OpenAI en backup.\n",
        "\n",
        "---\n",
        "\n",
        "👉 Cette organisation donne une **pipeline modulaire et robuste** :\n",
        "\n",
        "* **FAISS/Chroma** → mémoire locale.\n",
        "* **Tavily/Google** → compléments web.\n",
        "* **Groq** → moteur rapide pour synthèse finale.\n",
        "* **Streamlit** → interface utilisateur claire.\n",
        "\n",
        "---\n",
        "\n",
        "Veux-tu que je t’écrive un **exemple minimal complet** (un seul fichier `agentic_rag.py` + `app.py`) que tu peux exécuter immédiatement dans VSCode pour valider que tout tourne ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7607c300",
      "metadata": {
        "id": "7607c300"
      },
      "source": [
        "**Notebook Jupyter (`agentic_rag.ipynb`) complet** :\n",
        "\n",
        "* **Explications pas à pas**\n",
        "* **Code prêt à exécuter sous Windows 10 + Python 3.11 + VSCode**\n",
        "* Intégration **Groq (LLM)** + **FAISS (index local)** + **Tavily (recherche web)**\n",
        "* Un test de bout en bout avec Streamlit\n",
        "\n",
        "---\n",
        "\n",
        "# 📘 `agentic_rag.ipynb`\n",
        "\n",
        "```markdown\n",
        "# Agentic RAG avec Groq, FAISS et Tavily\n",
        "\n",
        "Ce notebook montre comment construire une application RAG (Retrieval-Augmented Generation)\n",
        "utilisant :\n",
        "- **Groq (LLM Llama3)** pour générer les réponses\n",
        "- **FAISS** comme index vectoriel pour stocker/retrouver des documents\n",
        "- **Tavily** comme moteur de recherche externe\n",
        "- **Streamlit** pour l’interface utilisateur\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "# =====================================\n",
        "# 1. Installation des dépendances\n",
        "# =====================================\n",
        "!pip install streamlit langchain langchain-groq langchain-community \\\n",
        "             langchain-huggingface langchain-tavily faiss-cpu \\\n",
        "             sentence-transformers python-dotenv\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "```markdown\n",
        "## 2. Configuration des clés API\n",
        "\n",
        "- Récupère une clé **Groq** sur [https://console.groq.com/keys](https://console.groq.com/keys)  \n",
        "- Récupère une clé **Tavily** sur [https://tavily.com](https://tavily.com)  \n",
        "\n",
        "⚠️ Pour ce notebook, nous mettons les clés **en dur** (pas de `.env`).\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "# =====================================\n",
        "# 2. Clés API\n",
        "# =====================================\n",
        "\n",
        "# ⚠️ Mets tes vraies clés ici\n",
        "GROQ_API_KEY = \"gsk_ta_vraie_cle\"\n",
        "TAVILY_API_KEY = \"tvly_ta_vraie_cle\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "```markdown\n",
        "## 3. Initialisation du modèle Groq\n",
        "\n",
        "On utilise **Llama3-70B** (grand modèle de Meta, hébergé par Groq).  \n",
        "C’est un modèle rapide et adapté au RAG.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(\n",
        "    groq_api_key=GROQ_API_KEY,\n",
        "    model=\"llama3-70b-8192\",  # ou \"llama3-8b-8192\" pour un modèle plus léger\n",
        "    temperature=0.2,\n",
        "    max_tokens=512\n",
        ")\n",
        "\n",
        "print(\"✅ LLM Groq initialisé\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "```markdown\n",
        "## 4. Embeddings et FAISS\n",
        "\n",
        "On utilise `HuggingFaceEmbeddings` (PyTorch only) + FAISS comme index vectoriel.  \n",
        "On commence par **créer un index** avec quelques documents d’exemple.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Embeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Documents simples pour tester\n",
        "docs = [\n",
        "    \"Llama3 est une famille de modèles de langage développée par Meta.\",\n",
        "    \"Llama3-70B est optimisé pour le raisonnement complexe.\",\n",
        "    \"Llama3-8B est plus petit, mais plus rapide et efficace.\"\n",
        "]\n",
        "\n",
        "# Construire et sauvegarder l’index FAISS\n",
        "vectorstore = FAISS.from_texts(docs, embeddings)\n",
        "vectorstore.save_local(\"faiss_index\")\n",
        "\n",
        "print(\"✅ Index FAISS créé et sauvegardé\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "```markdown\n",
        "## 5. Rechargement de FAISS et création de la chaîne RAG\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Charger l’index FAISS\n",
        "vectorstore = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    chain_type=\"stuff\",\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "print(\"✅ Chaîne RAG initialisée\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "```markdown\n",
        "## 6. Intégration de Tavily (recherche web)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "from langchain_tavily import TavilySearch\n",
        "\n",
        "tools = []\n",
        "if TAVILY_API_KEY and len(TAVILY_API_KEY) > 5:\n",
        "    tavily_tool = TavilySearch(api_key=TAVILY_API_KEY, max_results=3)\n",
        "    tools.append(tavily_tool)\n",
        "    print(\"✅ Tavily activé\")\n",
        "else:\n",
        "    print(\"ℹ️ Tavily désactivé (pas de clé API)\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "```markdown\n",
        "## 7. Création de l’agent combinant FAISS + Tavily\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "\n",
        "agent = None\n",
        "if tools:\n",
        "    agent = initialize_agent(\n",
        "        tools=tools,\n",
        "        llm=llm,\n",
        "        agent=AgentType.OPENAI_FUNCTIONS,\n",
        "        verbose=True,\n",
        "        handle_parsing_errors=True\n",
        "    )\n",
        "    print(\"✅ Agent RAG initialisé avec Tavily\")\n",
        "else:\n",
        "    print(\"ℹ️ Agent RAG initialisé uniquement avec FAISS\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "```markdown\n",
        "## 8. Pipeline complet Agentic RAG\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "def agentic_rag_pipeline(query: str) -> str:\n",
        "    response_text = \"\"\n",
        "\n",
        "    # Étape 1 : recherche dans l’index FAISS\n",
        "    if qa_chain:\n",
        "        try:\n",
        "            result = qa_chain({\"query\": query})\n",
        "            response_text += f\"Réponse basée sur documents :\\n{result['result']}\\n\\n\"\n",
        "            if \"source_documents\" in result:\n",
        "                sources = [doc.metadata.get(\"source\", \"inconnu\") for doc in result[\"source_documents\"]]\n",
        "                response_text += f\"Sources : {sources}\\n\\n\"\n",
        "        except Exception as e:\n",
        "            response_text += f\"⚠️ Erreur FAISS : {e}\\n\\n\"\n",
        "\n",
        "    # Étape 2 : recherche web (si Tavily activé)\n",
        "    if agent:\n",
        "        try:\n",
        "            web_info = agent.run(query)\n",
        "            response_text += f\"Infos Web (Tavily) :\\n{web_info}\\n\"\n",
        "        except Exception as e:\n",
        "            response_text += f\"⚠️ Erreur Tavily : {e}\\n\"\n",
        "\n",
        "    if response_text.strip() == \"\":\n",
        "        response_text = \"⚠️ Aucune réponse trouvée\"\n",
        "\n",
        "    return response_text\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "```markdown\n",
        "## 9. Test du pipeline\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "query = \"Explique-moi le rôle des modèles Llama3\"\n",
        "print(agentic_rag_pipeline(query))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "```markdown\n",
        "## 10. Intégration avec Streamlit\n",
        "\n",
        "On crée maintenant un fichier `app.py` minimal qui connecte ce pipeline à une UI Streamlit.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from agentic_rag import agentic_rag_pipeline\n",
        "\n",
        "st.set_page_config(page_title=\"Agentic RAG avec Groq\", layout=\"wide\")\n",
        "\n",
        "st.title(\"🔎 Agentic RAG (Groq + FAISS + Tavily)\")\n",
        "\n",
        "query = st.text_area(\"Pose ta question :\", \"Explique-moi le rôle des modèles Llama3\")\n",
        "\n",
        "if st.button(\"Envoyer\"):\n",
        "    with st.spinner(\"L'agent réfléchit...\"):\n",
        "        try:\n",
        "            response = agentic_rag_pipeline(query)\n",
        "            st.success(\"Réponse :\")\n",
        "            st.write(response)\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur : {e}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "````markdown\n",
        "## 11. Lancer l’app Streamlit\n",
        "\n",
        "```bash\n",
        "streamlit run app.py\n",
        "````\n",
        "\n",
        "Puis ouvre [http://localhost:8501](http://localhost:8501).\n",
        "\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "👉 Ce notebook te donne **tout le flux complet** :  \n",
        "- Création FAISS  \n",
        "- Chargement RAG  \n",
        "- Ajout Tavily  \n",
        "- Agent complet  \n",
        "- Test en mode notebook  \n",
        "- Export vers `app.py` pour Streamlit  \n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d56897c2",
      "metadata": {
        "id": "d56897c2"
      },
      "source": [
        "# Bilan"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2041db35",
      "metadata": {
        "id": "2041db35"
      },
      "source": [
        "\n",
        "###  Réalisations principales\n",
        "\n",
        "* **LLM Groq (Llama3-70B/8B)** intégré avec LangChain.\n",
        "* **Retriever local (FAISS)** créé, sauvegardé, rechargé.\n",
        "* **Chaîne RAG (`RetrievalQA`)** avec attribution de sources.\n",
        "* **Outil externe (Tavily)** ajouté pour la recherche web.\n",
        "* **Agent** combinant FAISS + Tavily + Groq via LangChain.\n",
        "* **Pipeline Python clair** (`agentic_rag_pipeline`) réutilisable.\n",
        "* **Interface Streamlit** pour poser des questions interactives.\n",
        "* **Gestion d’erreurs** (FAISS absent, Tavily non configuré).\n",
        "\n",
        "---\n",
        "\n",
        "###  Points optionnels ou restants\n",
        "\n",
        "* **LangSmith tracing** non intégré (optionnel dans l’exercice).\n",
        "* **Indexation avancée** à partir de vrais documents `.pdf/.txt` à compléter avec un script dédié.\n",
        "* **Migration à jour des libs** : `langchain-huggingface` et `langchain-tavily` (les anciennes classes sont dépréciées).\n",
        "\n",
        "---\n",
        "\n",
        "###  Conclusion\n",
        "\n",
        "L’exercice est **répondu dans l’ensemble** :\n",
        "\n",
        "* tu as couvert tous les objectifs essentiels (retriever, agent, orchestration, pipeline, Streamlit).\n",
        "* seuls les raffinements optionnels restent à ajouter si tu veux aller en mode production.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59410653",
      "metadata": {
        "id": "59410653"
      },
      "source": [
        "# Projets potentiels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f6802fd",
      "metadata": {
        "id": "4f6802fd"
      },
      "source": [
        "À partir des concepts que tu as travaillés dans cet exercice (RAG, agents, intégration d’outils, Streamlit), voici **5 projets innovants, créatifs et concrets** :\n",
        "\n",
        "---\n",
        "\n",
        "## 1. **Assistant juridique augmenté**\n",
        "\n",
        "* **Concept** : un agent qui lit les textes de loi (indexés en FAISS) et qui complète par une recherche Tavily pour vérifier les jurisprudences récentes.\n",
        "* **Innovation** : combine droit « statique » (loi en vigueur) et droit « dynamique » (actualités, jurisprudence).\n",
        "* **Concret** : avocat ou étudiant en droit qui tape une question → réponse avec article exact + résumé de cas récents.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **Coach santé personnalisé**\n",
        "\n",
        "* **Concept** : indexer des articles scientifiques en nutrition et sport, les combiner avec Tavily pour récupérer les dernières recommandations (OMS, revues médicales).\n",
        "* **Innovation** : un assistant qui donne des conseils avec **attribution scientifique** (sources fiables).\n",
        "* **Concret** : utilisateur pose « Quel régime est adapté au prédiabète ? » → réponse structurée avec références réelles.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. **Consultant entreprise « veille stratégique »**\n",
        "\n",
        "* **Concept** : index interne des rapports annuels, emails, notes stratégiques + Tavily pour compléter avec les actualités du secteur.\n",
        "* **Innovation** : assistant qui génère une **synthèse stratégique contextualisée** (forces/faiblesses + tendances externes).\n",
        "* **Concret** : un dirigeant tape « Quels sont les risques 2025 pour notre secteur ? » → l’agent croise infos internes + articles récents.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. **Tuteur éducatif interactif**\n",
        "\n",
        "* **Concept** : base locale (cours PDF, polycopiés) + recherche web académique pour ajouter des exemples récents.\n",
        "* **Innovation** : génère des **explications adaptées au niveau** (lycée, master, professionnel) et des quiz dynamiques.\n",
        "* **Concret** : étudiant tape « Explique la dérivée logarithmique au niveau lycée » → réponse + mini-exercices générés.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. **Assistant culturel et patrimonial**\n",
        "\n",
        "* **Concept** : indexation de guides touristiques, catalogues de musées, archives historiques + recherche web pour actualités (expos temporaires, événements).\n",
        "* **Innovation** : agent « guide intelligent » qui contextualise une visite en temps réel (histoire + actu culturelle).\n",
        "* **Concret** : touriste demande « Raconte-moi l’histoire de Notre-Dame et les événements actuels » → mélange archives + dernières nouvelles.\n",
        "\n",
        "---\n",
        "\n",
        "👉 Ces 5 projets reprennent **la même base technique** (Groq + FAISS + Tavily + Streamlit) mais appliqués à des cas **fortement différenciés** : droit, santé, entreprise, éducation, culture.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}