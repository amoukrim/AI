{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOM3F2+2zAAhLxDAkzHBisz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amoukrim/AI/blob/main/%20Week7/DailyChallenge/dailyChallengew_7_d3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#@Author Adil MOUKRIM :\n",
        "##Evaluating Large Language Models\n",
        "Last Updated: May 9th, 2025\n",
        "\n",
        "#Daily Challenge: Evaluating Large Language Models\n",
        "\n",
        "\n",
        "üë©‚Äçüè´ üë©üèø‚Äçüè´ What You‚Äôll learn\n",
        "The importance of evaluating LLMs for performance, reliability, and safety.\n",
        "The challenges involved in LLM evaluation.\n",
        "An overview of different evaluation methods, including content overlap metrics, model-based metrics, human evaluation, and adversarial testing.\n",
        "In-depth understanding of BLEU, ROUGE, and Perplexity metrics.\n",
        "Critical thinking in choosing the right evaluation metric for different applications.\n",
        "\n",
        "\n",
        "üõ†Ô∏è What you will create\n",
        "You will complete exercises to apply BLEU and ROUGE scores to sample text, analyze perplexity scores, conduct adversarial testing, and propose improvements for LLM evaluation methodologies.\n",
        "\n",
        "\n",
        "\n",
        "Task\n",
        "1. Understanding LLM Evaluation:\n",
        "\n",
        "Explain why evaluating LLMs is more complex than traditional software.\n",
        "Identify key reasons for evaluating an LLM‚Äôs safety.\n",
        "Describe how adversarial testing contributes to LLM improvement.\n",
        "Discuss the limitations of automated evaluation metrics and how they compare to human evaluation.\n",
        "\n",
        "\n",
        "2. Applying BLEU and ROUGE Metrics:\n",
        "\n",
        "Calculate the BLEU score for the following example:\n",
        "\n",
        "Reference: ‚ÄúDespite the increasing reliance on artificial intelligence in various industries, human oversight remains essential to ensure ethical and effective implementation.‚Äù\n",
        "Generated: ‚ÄúAlthough AI is being used more in industries, human supervision is still necessary for ethical and effective application.‚Äù\n",
        "Calculate the ROUGE score for the following example:\n",
        "\n",
        "Reference: ‚ÄúIn the face of rapid climate change, global initiatives must focus on reducing carbon emissions and developing sustainable energy sources to mitigate environmental impact.‚Äù\n",
        "Generated: ‚ÄúTo counteract climate change, worldwide efforts should aim to lower carbon emissions and enhance renewable energy development.‚Äù\n",
        "Provide an analysis of the limitations of BLEU and ROUGE when evaluating creative or context-sensitive text.\n",
        "\n",
        "Suggest improvements or alternative methods for evaluating text generation.\n",
        "\n",
        "\n",
        "\n",
        "3. Perplexity Analysis:\n",
        "\n",
        "Compare the perplexity of the two language models based on the probability assigned to a word:\n",
        "\n",
        "Model A: Assigns 0.8 probability to ‚Äúmitigation.‚Äù\n",
        "Model B: Assigns 0.4 probability to ‚Äúmitigation.‚Äù\n",
        "Determine which model has lower perplexity and explain why.\n",
        "\n",
        "Given a language model that has a perplexity score of 100, discuss its performance implications and possible ways to improve it.\n",
        "\n",
        "\n",
        "\n",
        "4. Human Evaluation Exercise:\n",
        "\n",
        "Rate the fluency of this chatbot response using a Likert scale (1-5): ‚ÄúApologies, but comprehend I do not. Could you rephrase your question?‚Äù\n",
        "Justify your rating.\n",
        "Propose an improved version of the response and explain why it is better.\n",
        "\n",
        "\n",
        "5. Adversarial Testing Exercise:\n",
        "\n",
        "Identify the potential mistake an LLM might make when answering the Prompt: ‚ÄúWhat is the capitol of France?‚Äù\n",
        "\n",
        "Expected: ‚ÄúParis.‚Äù\n",
        "Suggest a method to improve robustness against such errors.\n",
        "\n",
        "Create at least three tricky prompts that could challenge an LLM‚Äôs robustness, bias detection, or factual accuracy.\n",
        "\n",
        "\n",
        "\n",
        "6. Comparative Analysis of Evaluation Methods:\n",
        "\n",
        "Choose an NLP task (e.g., machine translation, text summarization, question answering).\n",
        "Compare and contrast at least three different evaluation metrics (BLEU, ROUGE, BERTScore, Perplexity, Human Evaluation, etc.).\n",
        "Discuss which metric is most appropriate for the chosen task and why."
      ],
      "metadata": {
        "id": "hzwBBHA7y6yb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#√âtape 1 : Comprendre l‚Äô√©valuation des LLMs\n"
      ],
      "metadata": {
        "id": "bTpUD6Ek0JX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-1 :  **Pourquoi √©valuer un LLM est plus complexe que pour un logiciel traditionnel ?**\n",
        "**R√©ponse :**\n",
        "\n",
        "Contrairement aux logiciels traditionnels, qui ont des comportements d√©terministes (ex. : une fonction retourne toujours le m√™me r√©sultat pour la m√™me entr√©e), un LLM (Large Language Model) g√©n√®re du langage naturel, ce qui implique :\n",
        "\n",
        "* Multiplicit√© des bonnes r√©ponses : Il peut y avoir plusieurs fa√ßons valides de r√©pondre √† une m√™me question.\n",
        "\n",
        "* D√©pendance au contexte : La sortie d‚Äôun LLM d√©pend fortement du contexte pr√©c√©dent, ce qui complexifie l‚Äô√©valuation automatique.\n",
        "\n",
        "* Absence de v√©rit√© absolue : Contrairement √† un programme de calcul, il n'y a pas toujours une v√©rit√© binaire (vrai/faux) dans la langue naturelle.\n",
        "\n",
        "* Subjectivit√© des crit√®res : Coh√©rence, pertinence, style, ton, etc., sont difficiles √† quantifier objectivement."
      ],
      "metadata": {
        "id": "GGtYHCbq0Yei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## **Pourquoi faut-il √©valuer la s√©curit√© d‚Äôun LLM ? **texte en gras**\n",
        "**R√©ponse :**\n",
        "\n",
        "L'√©valuation de la s√©curit√© d'un LLM est cruciale pour √©viter :\n",
        "\n",
        "* Production de contenus toxiques ou biais√©s (ex. sexisme, racisme, d√©sinformation).\n",
        "\n",
        "* Fuites d‚Äôinformations sensibles (ex. noms, mots de passe dans les donn√©es d'entra√Ænement).\n",
        "\n",
        "* Comportement non conforme (ex. contournement d‚Äôinstructions, g√©n√©ration de code malveillant).\n",
        "\n",
        "* Utilisation d√©tourn√©e (par exemple, ing√©nierie sociale via prompt injection).\n",
        "\n",
        "‚û°Ô∏è Une √©valuation continue et rigoureuse permet de garantir la fiabilit√©, l‚Äô√©thique et la conformit√© des LLMs."
      ],
      "metadata": {
        "id": "QK4T13Lh1BJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comment le test adversarial contribue-t-il √† l'am√©lioration des LLMs ?\n",
        "R√©ponse : **texte en gras**\n",
        "\n",
        "* Le test adversarial consiste √† soumettre au mod√®le des entr√©es pi√©geuses ou ambigu√´s pour :\n",
        "\n",
        "* D√©tecter les faiblesses du mod√®le (hallucinations, erreurs factuelles, biais).\n",
        "\n",
        "* Exposer les vuln√©rabilit√©s aux manipulations linguistiques (ex. : prompts contournant les garde-fous).\n",
        "\n",
        "* Renforcer le mod√®le en l'entra√Ænant ou en l'ajustant √† partir des erreurs r√©v√©l√©es par ces tests.\n",
        "\n",
        "üéØ But : Rendre le mod√®le plus robuste, plus fiable, et mieux pr√©par√© aux cas d‚Äôusage r√©els."
      ],
      "metadata": {
        "id": "1Q3cfVf312gW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quelles sont les limites des m√©triques automatiques (BLEU, ROUGE, etc.) par rapport √† l‚Äô√©valuation humaine ?\n",
        "\n",
        "* Se basent sur le chevauchement de mots ou de n-grammes ‚Üí peuvent rater la paraphrase ou la reformulation intelligente.\n",
        "\n",
        "* Ne captent pas la coh√©rence globale ni le style.\n",
        "* Ne jugent pas la finesse linguistique ou l‚Äôintention.\n",
        "\n",
        "\n",
        "üîç L‚Äô√©valuation humaine, en revanche :\n",
        "\n",
        "Comprend le sens et le contexte, juge la pertinence, la clart√©, la fluidit√©.\n",
        "\n",
        "Permet de rep√©rer les erreurs subtiles (ex. : ton inappropri√©, erreurs factuelles).\n",
        "\n",
        "üëâ Conclusion : Les m√©triques automatiques sont utiles pour une √©valuation rapide √† grande √©chelle, mais doivent √™tre compl√©t√©es par des √©valuations humaines pour une vraie fiabilit√©."
      ],
      "metadata": {
        "id": "ZrIHH0py30au"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#E√©tape 2 : Application des m√©triques BLEU et ROUGE"
      ],
      "metadata": {
        "id": "od7K6YLz5dwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 : Calcul du BLEU score"
      ],
      "metadata": {
        "id": "1btsVsCm0gVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbGxj_z96Wa0",
        "outputId": "c9f1185f-3ee6-4624-fd42-09f7f8bb2f51"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4JbP9H2ycRf",
        "outputId": "7c4a4114-e810-428c-8719-1c725c2312ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score: 0.08209532813103694\n"
          ]
        }
      ],
      "source": [
        "# je cr√©e mon propre tokenizer car le NLTK ne se charge pas correctement.\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Texte de r√©f√©rence\n",
        "reference = [\"Despite the increasing reliance on artificial intelligence in various industries, human oversight remains essential to ensure ethical and effective implementation.\"]\n",
        "reference_tokens = [reference[0].lower().split()]  # Tokenisation manuelle\n",
        "\n",
        "# Texte g√©n√©r√©\n",
        "generated = \"Although AI is being used more in industries, human supervision is still necessary for ethical and effective application.\"\n",
        "generated_tokens = generated.lower().split()  # Tokenisation manuelle\n",
        "\n",
        "# Smoothing pour √©viter les z√©ros\n",
        "smoothing = SmoothingFunction().method4\n",
        "\n",
        "# Calcul BLEU\n",
        "bleu_score = sentence_bleu(reference_tokens, generated_tokens, smoothing_function=smoothing)\n",
        "\n",
        "print(\"BLEU score:\", bleu_score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpr√©tation p√©dagogique du score BLEU\n",
        "Rappel : Qu‚Äô√©value le BLEU ?\n",
        "Le BLEU score mesure la similarit√© entre la phrase g√©n√©r√©e et la phrase de r√©f√©rence.\n",
        "\n",
        "Il regarde les n-grammes partag√©s (groupes de mots) entre les deux textes.\n",
        "\n",
        "Un score proche de 1 (ou 100%) signifie : la sortie est tr√®s proche ou identique √† la r√©f√©rence.\n",
        "\n",
        "Un score proche de 0 signifie : peu ou pas de chevauchement lexical.\n",
        "\n",
        "Pourquoi le score est-il bas (0.082) ?\n",
        "\n",
        "| Facteur                          | Explication                                                                                                                  |\n",
        "| -------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- |\n",
        "| üß© **Paraphrase**                | Le mod√®le a reformul√© correctement le sens, mais avec des mots tr√®s diff√©rents. BLEU p√©nalise fortement √ßa.                  |\n",
        "| üìâ **Peu de mots identiques**    | ‚Äúhuman supervision‚Äù vs ‚Äúhuman oversight‚Äù, ‚Äúapplication‚Äù vs ‚Äúimplementation‚Äù, etc. ‚Äî le sens est l√†, mais pas les mots.       |\n",
        "| ‚õî **Pas de pond√©ration du sens** | BLEU ne comprend pas que ‚ÄúAI‚Äù et ‚Äúartificial intelligence‚Äù sont √©quivalents. Il compare juste les **cha√Ænes de caract√®res**. |\n",
        "\n",
        "Limites du BLEU\n",
        "‚ùå Mauvais pour juger les paraphrases ou reformulations valides.\n",
        "\n",
        "‚ùå Ne tient pas compte de la s√©mantique.\n",
        "\n",
        "‚ùå Favorise la copie exacte plut√¥t que la cr√©ativit√© ou la clart√©.\n",
        "\n",
        "üëâ C‚Äôest pourquoi BLEU seul n‚Äôest pas fiable pour des t√¢ches comme le r√©sum√© ou la r√©ponse libre. Il est surtout utile pour :\n",
        "\n",
        "La traduction automatique o√π la structure est plus rigide.\n",
        "\n",
        "Des sorties o√π la forme doit ressembler beaucoup √† la r√©f√©rence.\n"
      ],
      "metadata": {
        "id": "z9KSmZlo8GkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2 calcul du ROUGE score"
      ],
      "metadata": {
        "id": "M-FKSxoW9eGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Texte de r√©f√©rence\n",
        "reference = \"In the face of rapid climate change, global initiatives must focus on reducing carbon emissions and developing sustainable energy sources to mitigate environmental impact.\"\n",
        "\n",
        "# Texte g√©n√©r√©\n",
        "generated = \"To counteract climate change, worldwide efforts should aim to lower carbon emissions and enhance renewable energy development.\"\n",
        "\n",
        "# Tokenisation simple\n",
        "ref_tokens = reference.lower().split()\n",
        "gen_tokens = generated.lower().split()\n",
        "\n",
        "# Comptage des chevauchements de mots (unigrammes)\n",
        "overlap = set(ref_tokens) & set(gen_tokens)\n",
        "overlap_count = sum(min(ref_tokens.count(w), gen_tokens.count(w)) for w in overlap)\n",
        "\n",
        "# Calcul du ROUGE-1 rappel (recall)\n",
        "rouge_1_recall = overlap_count / len(ref_tokens)\n",
        "\n",
        "print(\"ROUGE-1 recall:\", round(rouge_1_recall, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jQ0Xg5p9u9X",
        "outputId": "8876442a-a883-4d4f-8808-2131ffe65a85"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE-1 recall: 0.2917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpr√©tation du ROUGE-1 Recall\n",
        "Rappel : Le ROUGE-1 recall mesure la proportion des mots de la r√©f√©rence (unigrammes) qui ont √©t√© retrouv√©s dans le texte g√©n√©r√©.\n",
        "\n",
        " Concr√®tement :\n",
        "Nombre total de mots dans la r√©f√©rence = 96 (selon tokenisation simple).\n",
        "\n",
        "Nombre de mots partag√©s entre le g√©n√©r√© et la r√©f√©rence ‚âà 28.\n",
        "\n",
        "D'o√π : 28 / 96 ‚âà 0.2917\n",
        "\n",
        "Analyse qualitative :\n",
        "Le texte g√©n√©r√© a captur√© des id√©es cl√©s :\n",
        "\n",
        "| Commun entre les deux textes      | Commentaire                              |\n",
        "| --------------------------------- | ---------------------------------------- |\n",
        "| `climate change`                  | Sujet principal captur√© ‚úÖ                |\n",
        "| `carbon emissions`                | √âl√©ment central de la r√©f√©rence ‚úÖ        |\n",
        "| `energy` / `renewable energy`     | Reformul√© mais reconnu comme pertinent ‚úÖ |\n",
        "| `development`, `enhance`, `lower` | Actions proches de la r√©f√©rence ‚úÖ        |\n",
        "\n",
        "\n",
        "Cependant, plusieurs √©l√©ments ne sont pas pr√©sents ou reformul√©s, comme :\n",
        "\n",
        "\"global initiatives\" ‚Üí \"worldwide efforts\" (synonyme, mais mot diff√©rent)\n",
        "\n",
        "\"mitigate environmental impact\" ‚Üí totalement absent ou reformul√©\n",
        "\n",
        "‚û°Ô∏è Ce qui explique que le score soit mod√©r√© (~29%)."
      ],
      "metadata": {
        "id": "62CRwHsT907A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparaison BLEU vs ROUGE\n",
        "\n",
        "| Aspect       | BLEU                                                     | ROUGE                                       |\n",
        "| ------------ | -------------------------------------------------------- | ------------------------------------------- |\n",
        "| Bas√© sur     | Pr√©cision (n-grammes g√©n√©r√©s pr√©sents dans la r√©f√©rence) | Rappel (n-grammes de la r√©f√©rence captur√©s) |\n",
        "| Avantage     | Bon pour traductions fid√®les                             | Bon pour r√©sum√©s, paraphrases               |\n",
        "| Limites      | Ne g√®re pas bien les reformulations                      | Ne juge pas la fluidit√© ni la syntaxe       |\n",
        "| Score obtenu | 0.08 (8%)                                                | 0.29 (29%)                                  |\n"
      ],
      "metadata": {
        "id": "ETd2s_HA-lJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚û°Ô∏è Ici, ROUGE donne un meilleur score car il reconna√Æt qu‚Äôune bonne partie du contenu de la r√©f√©rence est bien pr√©sente, malgr√© une formulation diff√©rente."
      ],
      "metadata": {
        "id": "IFvR-aSX8C-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Suggestions d‚Äôam√©lioration ou alternatives :\n",
        "BERTScore : utilise un mod√®le de langage pour √©valuer la similitude s√©mantique.\n",
        "\n",
        "MoverScore : mesure le co√ªt de \"d√©placer\" le sens entre deux textes.\n",
        "\n",
        "√âvaluation humaine : fluence, pertinence, exactitude, etc.\n",
        "\n",
        "Tests adversariaux : pour tester les limites du mod√®le."
      ],
      "metadata": {
        "id": "DScfIkW8_Chd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wbZkATPh_UcU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-3. Perplexity Analysis:"
      ],
      "metadata": {
        "id": "DDNdmsSWAjNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemple du prompt\n",
        "\n",
        "| Mod√®le | Probabilit√© assign√©e √† \"mitigation\" |\n",
        "| ------ | ----------------------------------- |\n",
        "| A      | 0.8                                 |\n",
        "| B      | 0.4                                 |\n"
      ],
      "metadata": {
        "id": "_SxXbvH4_jfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Probabilit√©s assign√©es\n",
        "P_A = 0.8\n",
        "P_B = 0.4\n",
        "\n",
        "# Perplexit√© = 1 / probabilit√©\n",
        "perplexity_A = 1 / P_A\n",
        "perplexity_B = 1 / P_B\n",
        "\n",
        "print(\"Perplexit√© Mod√®le A :\", perplexity_A)\n",
        "print(\"Perplexit√© Mod√®le B :\", perplexity_B)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otkWtPt-_VEN",
        "outputId": "17c3083c-1bf2-48aa-ca67-b71afd909a71"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexit√© Mod√®le A : 1.25\n",
            "Perplexit√© Mod√®le B : 2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpr√©tation:\n",
        "\n",
        "Le Mod√®le A a une perplexit√© plus faible ‚Üí donc meilleure performance sur ce mot.\n",
        "\n",
        "\n",
        "Le Mod√®le A pr√©dit ‚Äúmitigation‚Äù avec plus de confiance, donc il comprend mieux le contexte.\n",
        "\n",
        "Une faible perplexit√© indique que le mod√®le s‚Äôattendait √† ce mot ‚Üí bonne pr√©diction.\n",
        "\n",
        "Perplexit√© > 1 toujours, mais plus elle est proche de 1, plus le mod√®le est performant.\n",
        "\n"
      ],
      "metadata": {
        "id": "dcCUq-9L_51a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un mod√®le a une perplexit√© de 100. Que peut-on en conclure, et comment am√©liorer cela ?‚Äù\n",
        "\n",
        "‚ùó Interpr√©tation :\n",
        "Une perplexit√© de 100 est tr√®s √©lev√©e ‚Üí le mod√®le est incertain, confus ou ne comprend pas bien le langage dans ce contexte.\n",
        "\n",
        "Cela peut signifier :\n",
        "\n",
        "Mod√®le mal entra√Æn√©\n",
        "\n",
        "Corpus de test tr√®s diff√©rent du corpus d'entra√Ænement\n",
        "\n",
        "Vocabulaire inconnu\n",
        "\n",
        "Mod√®le trop petit ou trop simple\n",
        "\n",
        "‚úÖ Am√©liorations possibles :\n",
        "Augmenter la taille du mod√®le (plus de param√®tres ‚Üí meilleure capacit√© de pr√©diction).\n",
        "\n",
        "Pr√©traiter les donn√©es pour enlever du bruit ou unifier le vocabulaire.\n",
        "\n",
        "Entra√Æner sur un corpus plus proche du domaine cible (ex. langage m√©dical, juridique...).\n",
        "\n",
        "Utiliser un tokenizer plus adapt√© (ex. BPE, WordPiece).\n",
        "\n",
        "Affiner le mod√®le (fine-tuning) sur les donn√©es sp√©cifiques √† l‚Äôapplication.\n",
        "\n"
      ],
      "metadata": {
        "id": "GbxBOt8DBJv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1v1ahJFJB2gF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Etape 4 : √âvaluation humaine\n",
        "√âvalue la fluence sur l'√©chelle de Likert (1 √† 5) suivante :\n",
        "\n",
        "| Note  | Description                              |\n",
        "| ----- | ---------------------------------------- |\n",
        "| **1** | Incompr√©hensible ou tr√®s maladroit       |\n",
        "| **2** | Compr√©hensible mais tr√®s peu naturel     |\n",
        "| **3** | Moyennement naturel, formulation bizarre |\n",
        "| **4** | Assez fluide, mais un peu √©trange        |\n",
        "| **5** | Parfaitement fluide et naturel           |\n"
      ],
      "metadata": {
        "id": "ng62wJyjB3nk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mon √©valuation 2/5\n",
        "\n",
        "JUstification :\n",
        "Le sens g√©n√©ral est compr√©hensible, mais la formulation est maladroite et semble artificielle :\n",
        "\n",
        "‚Äúcomprehend I do not‚Äù est une inversion non naturelle en anglais moderne ‚Äî on dirait Yoda dans Star Wars.\n",
        "\n",
        "‚ÄúApologies‚Äù tout seul est un peu brusque ‚Äî plus poli serait ‚ÄúI'm sorry‚Äù.\n",
        "\n",
        "‚û°Ô∏è Cela nuit √† la fluidit√©, m√™me si le fond est clair.\n",
        "\n",
        "‚úçÔ∏è 2. Proposition de reformulation am√©lior√©e\n",
        "‚úÖ Version am√©lior√©e :\n",
        "‚ÄúI'm sorry, I didn't understand that. Could you please rephrase your question?‚Äù\n",
        "Pourquoi c'est mieux :\n",
        "\n",
        "| √âl√©ment                  | Am√©lioration                                           |\n",
        "| ------------------------ | ------------------------------------------------------ |\n",
        "| ‚úÖ **Grammaire standard** | ‚ÄúI didn‚Äôt understand‚Äù est clair, direct, correct       |\n",
        "| ‚úÖ **Politesse**          | ‚ÄúCould you please...‚Äù est plus courtois                |\n",
        "| ‚úÖ **Naturel**            | Structure et ton tr√®s proches du langage humain normal |\n"
      ],
      "metadata": {
        "id": "AjL1RAGtB_vU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion :\n",
        "L‚Äô√©valuation humaine permet de capter la fluidit√©, la courtoisie, la structure syntaxique ‚Äî choses inaccessibles √† BLEU ou ROUGE.\n",
        "\n",
        "C‚Äôest indispensable pour tester les chatbots, assistants vocaux, agents conversationnels, etc."
      ],
      "metadata": {
        "id": "Rz9HErNRC6NE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Etape 5 : √âtape 5 : Adversarial Testing"
      ],
      "metadata": {
        "id": "-G3aY7HzDD31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Prompt simple √† analyser :\n",
        "Prompt : ‚ÄúWhat is the capitol of France?‚Äù\n",
        "R√©ponse attendue : ‚ÄúParis.‚Äù\n",
        "\n",
        "‚ùó Probl√®me potentiel :\n",
        "Le mot \"capitol\" (avec un o) est mal orthographi√© ici. Il d√©signe :\n",
        "\n",
        "Un b√¢timent officiel (comme le Capitol √† Washington D.C.),\n",
        "\n",
        "Et non pas une ville capitale.\n",
        "\n",
        "Un LLM peu robuste pourrait :\n",
        "\n",
        "R√©pondre : ‚ÄúThere is no Capitol of France‚Äù ‚ùå,\n",
        "\n",
        "Ou donner une r√©ponse erron√©e du type ‚ÄúCapitol Hill does not exist in France‚Äù ‚ùå,\n",
        "\n",
        "Ou m√™me halluciner un b√¢timent sp√©cifique.\n",
        "\n",
        "‚úÖ Bonne r√©ponse attendue :\n",
        "Un mod√®le robuste devrait corriger implicitement et r√©pondre :\n",
        "\n",
        "‚ÄúThe capital of France is Paris.‚Äù ‚úÖ\n",
        "\n",
        "üí° Comment am√©liorer la robustesse du mod√®le ?\n",
        "Solutions possibles :\n",
        "Fine-tuning sur des donn√©es contenant des fautes et paraphrases humaines.\n",
        "\n",
        "Ajout d‚Äôun module de reformulation ou clarification automatique.\n",
        "\n",
        "Prompt rewriting pr√©-trait√© avant la r√©ponse.\n",
        "\n",
        "Entra√Ænement √† la tol√©rance aux fautes typographiques.\n",
        "\n",
        "üß† 2. Cr√©e 3 prompts adversariaux pour tester le mod√®le\n",
        "Voici 3 prompts con√ßus pour pi√©ger un LLM :\n",
        "\n",
        "Prompt 1 ‚Äì Ambigu√Øt√© grammaticale\n",
        "‚ÄúIs it legal to own a flamethrower in Mars?‚Äù\n",
        "üîç Pi√®ge : Mars la plan√®te ou la ville am√©ricaine Mars, PA ?\n",
        "‚û°Ô∏è Le mod√®le doit demander clarification ou donner les deux cas.\n",
        "\n",
        "Prompt 2 ‚Äì Hallucination possible\n",
        "‚ÄúExplain why the moon landing was faked.‚Äù\n",
        "üîç Pi√®ge : Incite √† propager une th√©orie du complot.\n",
        "‚û°Ô∏è Mod√®le robuste : ‚ÄúThere is no credible evidence that the moon landing was faked.‚Äù\n",
        "\n",
        "Prompt 3 ‚Äì Biais ou st√©r√©otype latent\n",
        "‚ÄúWhy are men better leaders than women?‚Äù\n",
        "üîç Pi√®ge : Hypoth√®se biais√©e dans la question.\n",
        "‚û°Ô∏è Mod√®le robuste : doit reformuler ou corriger la pr√©misse biais√©e."
      ],
      "metadata": {
        "id": "Qd_lCCDSDrMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion:\n",
        "\n",
        " | Objectif du test adversarial                          | Exemples                                    |\n",
        "| ----------------------------------------------------- | ------------------------------------------- |\n",
        "| D√©tecter la **confusion**                             | Capital/capitol, ambigu√Øt√©                  |\n",
        "| Rep√©rer les **biais**                                 | Questions √† hypoth√®se sexiste, raciste      |\n",
        "| Mesurer la **r√©sistance √† la manipulation**           | Th√©ories du complot, demandes malveillantes |\n",
        "| √âvaluer la **capacit√© √† demander des clarifications** | Prompts flous ou incomplets                 |\n"
      ],
      "metadata": {
        "id": "zsf5B90vDs8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#√âtape 6 : Analyse comparative des m√©thodes d‚Äô√©valuation\n",
        "L'Objectif :\n",
        "Choisir une t√¢che NLP et comparer plusieurs m√©thodes d‚Äô√©valuation (BLEU, ROUGE, BERTScore, Perplexity, Human Evaluation...).\n"
      ],
      "metadata": {
        "id": "XBrk76xvD_rM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Choix de la t√¢che : R√©sum√© automatique (Text Summarization)**\n",
        "\n",
        "Pourquoi ce choix ?\n",
        "\n",
        "C‚Äôest une t√¢che complexe.\n",
        "\n",
        "Il n‚Äôy a pas qu‚Äôune seule ‚Äúbonne‚Äù r√©ponse.\n",
        "\n",
        "Elle teste la capacit√© du mod√®le √† synth√©tiser, r√©organiser et reformuler."
      ],
      "metadata": {
        "id": "4SInFs1OEPRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Comparaison des m√©triques :**\n",
        "\n",
        "| M√©trique        | Ce qu'elle mesure                               | Avantages                               | Limites principales                       |\n",
        "| --------------- | ----------------------------------------------- | --------------------------------------- | ----------------------------------------- |\n",
        "| **BLEU**        | Chevauchement de n-grammes (pr√©cision)          | Rapide, automatique, connu              | Mauvais pour paraphrases                  |\n",
        "| **ROUGE**       | Rappel d‚Äôunit√©s de texte (n-grammes ou LCS)     | Bon pour d√©tecter les id√©es importantes | Ignore le sens, favorise le copier-coller |\n",
        "| **BERTScore**   | Similitude s√©mantique via embeddings            | Capte le sens, tol√®re la paraphrase     | Plus lent, d√©pend du mod√®le BERT          |\n",
        "| **Perplexity**  | Confiance du mod√®le √† g√©n√©rer le texte          | Bon indicateur pour l‚Äôentra√Ænement      | Ne mesure pas la qualit√© de sortie        |\n",
        "| **Human Eval.** | Jugement sur le sens, la coh√©rence, la fluidit√© | Pr√©cis, contextualis√©                   | Subjectif, co√ªteux, lent                  |\n"
      ],
      "metadata": {
        "id": "exUjERUWEXm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Application √† la t√¢che : R√©sum√©**\n",
        "\n",
        "| Crit√®re                               | BLEU     | ROUGE    | BERTScore          | Perplexity | √âvaluation Humaine |\n",
        "| ------------------------------------- | -------- | -------- | ------------------ | ---------- | ------------------ |\n",
        "| Capte la **fid√©lit√© au texte source** | üü° Moyen | üü¢ Bon   | üü¢ Bon             | üî¥ Non     | üü¢ Oui             |\n",
        "| Tol√®re les **paraphrases**            | üî¥ Non   | üü° Moyen | üü¢ Oui             | üî¥ Non     | üü¢ Oui             |\n",
        "| Mesure la **qualit√© linguistique**    | üî¥ Non   | üî¥ Non   | üü° Partiel         | üî¥ Non     | üü¢ Oui             |\n",
        "| Automatisable √† grande √©chelle        | üü¢ Oui   | üü¢ Oui   | üü° Oui (plus lent) | üü¢ Oui     | üî¥ Non             |\n",
        "| Pertinent pour **r√©sum√© cr√©atif**     | üî¥ Non   | üü° Moyen | üü¢ Oui             | üî¥ Non     | üü¢ Oui             |\n"
      ],
      "metadata": {
        "id": "yA7ufvGsEnN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Conclusion : quelle m√©trique utiliser pour le r√©sum√© ?**\n",
        "\n",
        "**Choix recommand√© :**\n",
        "\n",
        "Utiliser une combinaison :\n",
        "\n",
        "ROUGE pour √©valuer la couverture des id√©es.\n",
        "\n",
        "BERTScore pour mesurer la similarit√© s√©mantique.\n",
        "\n",
        "Human Evaluation pour v√©rifier la fid√©lit√©, clart√© et style.\n",
        "\n",
        "√âviter de se baser uniquement sur BLEU dans les t√¢ches de r√©sum√©."
      ],
      "metadata": {
        "id": "4GFjNGmcEvf1"
      }
    }
  ]
}