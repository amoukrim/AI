{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN60tRrhAdy3kMDL0saOZit",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amoukrim/AI/blob/main/Week6/DailyChallenge/dailyChallengeW_6_D3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##@Author : Adil MOUKRIM\n",
        "\n",
        "\n",
        "Build a (RAG) System\n",
        "Last Updated: July 10th, 2025\n",
        "\n",
        "Daily Challenge: Build a Retrieval Augmented Generation (RAG) System\n",
        "\n",
        "\n",
        "üë©‚Äçüè´ üë©üèø‚Äçüè´ What You‚Äôll learn\n",
        "Implement a Retrieval Augmented Generation (RAG) system using Langchain and Hugging Face.\n",
        "Load and process datasets using Hugging Face datasets and Langchain HuggingFaceDatasetLoader.\n",
        "Split documents into smaller chunks using Langchain RecursiveCharacterTextSplitter.\n",
        "Generate text embeddings using Hugging Face sentence-transformers and Langchain HuggingFaceEmbeddings.\n",
        "Create and utilize vector stores with Langchain FAISS for efficient document retrieval.\n",
        "Prepare and integrate a pre-trained Language Model (LLM) from Hugging Face transformers for question answering.\n",
        "Build a Retrieval QA Chain using Langchain RetrievalQA to answer questions based on retrieved documents.\n",
        "\n",
        "\n",
        "üõ†Ô∏è What you will create\n",
        "You will create a functional RAG system that can answer questions based on a dataset loaded from Hugging Face Datasets. This system will:\n",
        "\n",
        "Load the databricks/databricks-dolly-15k dataset.\n",
        "Index the dataset content into a vector store.\n",
        "Utilize a pre-trained question-answering model from Hugging Face.\n",
        "Answer user queries by retrieving relevant documents and using the LLM to generate answers.\n",
        "\n",
        "\n",
        "Mandatory : You must read this article before starting the exercise\n",
        "Faiss | LangChain\n",
        "\n",
        "\n",
        "\n",
        "Mandatory : You must watch these videos before starting the exercise\n",
        "\n",
        "\n",
        "PyTorch in 100 Seconds\n",
        "\n",
        "\n",
        "\n",
        "LangChain Explained in 13 Minutes\n",
        "\n",
        "\n",
        "\n",
        "Task\n",
        "Our task is to implement RAG using Langchain and Hugging Face!\n",
        "\n",
        "1. Set up your environment: : This ensures all the necessary tools are available to build the RAG system. Each library serves a specific role: Langchain handles the orchestration of components, transformers provide pre-trained models, sentence-transformers generate embeddings, datasets load sample data, and FAISS enables fast similarity searches.\n",
        "\n",
        "Open your terminal or notebook environment.\n",
        "Install all required libraries by running these commands:\n",
        "\n",
        "\n",
        "!pip install -q langchain\n",
        "!pip install -q torch\n",
        "!pip install -q transformers\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -q datasets\n",
        "!pip install -q faiss-cpu\n",
        "!pip install -U langchain-community\n",
        "\n",
        "\n",
        "2. Load the dataset: To provide the system with information to retrieve from, you‚Äôll load a real-world dataset. HuggingFaceDatasetLoader simplifies the process of accessing Hugging Face datasets and formatting them into documents that Langchain can process.\n",
        "\n",
        "before loading the dataset, run :\n",
        "pip install -Uq datasets\n",
        "Import HuggingFaceDatasetLoader from langchain.document_loaders.\n",
        "Specify the dataset name and content column:\n",
        "\n",
        "\n",
        "dataset_name = \"databricks/databricks-dolly-15k\"\n",
        "page_content_column = \"context\"\n",
        "\n",
        "\n",
        "Create a HuggingFaceDatasetLoader instance and load the data as documents:\n",
        "\n",
        "\n",
        "loader = HuggingFaceDatasetLoader(dataset_name, page_content_column)\n",
        "data = loader.load()\n",
        "print(data[:2]) # Optional: Print the first 2 entries to verify loading\n",
        "\n",
        "\n",
        "3. Split the documents: Language models have a limit on how much text they can process at once. Splitting large documents into smaller, overlapping chunks ensures that no important context is lost and that each piece of text is a manageable size for embedding and retrieval.\n",
        "\n",
        "Import RecursiveCharacterTextSplitter from langchain.text_splitter.\n",
        "Create a RecursiveCharacterTextSplitter instance with a chunk_size of 1000 and chunk_overlap of 150:\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "\n",
        "\n",
        "Split the loaded documents:\n",
        "\n",
        "\n",
        "docs = text_splitter.split_documents(data)\n",
        "print(docs[0]) # Optional: Print the first document chunk\n",
        "\n",
        "\n",
        "4. Embed the text: Text needs to be converted into numerical representations (embeddings) so that similar pieces of text can be found efficiently. Using a sentence-transformer model creates embeddings that capture semantic meaning, enabling effective retrieval later.\n",
        "\n",
        "Import HuggingFaceEmbeddings from langchain.embeddings.\n",
        "Define the model path, model configurations, and encoding options:\n",
        "\n",
        "\n",
        "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "model_kwargs = {'device':'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "\n",
        "\n",
        "Initialize HuggingFaceEmbeddings:\n",
        "\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "  model_name=modelPath,\n",
        "  model_kwargs=model_kwargs,\n",
        "  encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "\n",
        "(Optional) Test embedding creation:\n",
        "\n",
        "\n",
        "text = \"This is a test document.\"\n",
        "query_result = embeddings.embed_query(text)\n",
        "print(query_result[:3])\n",
        "\n",
        "\n",
        "5. Create a vector store: A vector store like FAISS indexes the embeddings, allowing fast and scalable similarity searches. This is how the system quickly finds relevant pieces of text when a query is made.\n",
        "\n",
        "Import FAISS from langchain.vectorstores.\n",
        "Create a FAISS vector store from the document chunks and embeddings:\n",
        "\n",
        "\n",
        "db = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "\n",
        "Note: This step might take some time depending on your dataset size.\n",
        "\n",
        "\n",
        "6. Prepare the LLM model: The Language Model is responsible for generating answers based on retrieved documents. Loading a pre-trained model and wrapping it in a Langchain pipeline makes it easy to integrate with the retrieval system.\n",
        "\n",
        "Import necessary classes from transformers and langchain:\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "\n",
        "Load the tokenizer and question-answering model:\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Intel/dynamic_tinybert\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"Intel/dynamic_tinybert\")\n",
        "\n",
        "\n",
        "Create a question-answering pipeline:\n",
        "\n",
        "\n",
        "model_name = \"Intel/dynamic_tinybert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True, max_length=512)\n",
        "Youtubeer = pipeline(\n",
        "  \"question-answering\",\n",
        "  model=model_name,\n",
        "  tokenizer=tokenizer,\n",
        "  return_tensors='pt'\n",
        ")\n",
        "\n",
        "\n",
        "Create a Langchain pipeline wrapper:\n",
        "\n",
        "\n",
        "llm = HuggingFacePipeline(\n",
        "  pipeline=Youtubeer,\n",
        "  model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
        ")\n",
        "\n",
        "\n",
        "7. Build the Retrieval QA Chain: The Retrieval QA Chain connects the retriever (which finds relevant documents) with the LLM (which generates answers). This chain enables the full RAG process, where the system retrieves helpful context and then answers the user‚Äôs query based on that context.\n",
        "\n",
        "Import RetrievalQA from langchain.chains.\n",
        "Create a retriever from your FAISS database:\n",
        "\n",
        "\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 4}) # Optional: You can adjust k for number of documents retrieved\n",
        "\n",
        "\n",
        "Build the RetrievalQA chain:\n",
        "\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"refine\", retriever=retriever, return_source_documents=False)\n",
        "\n",
        "\n",
        "8. Test your RAG system: Running a test query allows you to verify that all components are working together. This step ensures that documents are retrieved correctly and that the model generates meaningful answers based on the retrieved context.\n",
        "\n",
        "Define your question:\n",
        "\n",
        "\n",
        "question = \"What is cheesemaking?\"\n",
        "\n",
        "\n",
        "Run the QA chain and print the result:\n",
        "\n",
        "\n",
        "result = qa.run({\"query\": question})\n",
        "print(result) # Or print(result[\"result\"]) if the output is a dictionary"
      ],
      "metadata": {
        "id": "_PC88dcesZbs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úÖ √âtape 1 : Installation des biblioth√®ques"
      ],
      "metadata": {
        "id": "8kILyk2bsigc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmtGiKY-sYdO",
        "outputId": "bdea95e8-1db5-410a-db16-0a954a2d1abd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.68)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.5)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "# LangChain : Permet de composer les diff√©rentes briques d'un syst√®me d'IA.\n",
        "!pip install -q langchain\n",
        "\n",
        "# Torch : Librairie de base pour les mod√®les PyTorch (n√©cessaire aux mod√®les Hugging Face).\n",
        "!pip install -q torch\n",
        "\n",
        "# Transformers : Fournit les mod√®les pr√©-entra√Æn√©s (comme BERT, GPT, etc.).\n",
        "!pip install -q transformers\n",
        "\n",
        "# Sentence-transformers : Permet de g√©n√©rer des embeddings de phrases pour la recherche s√©mantique.\n",
        "!pip install -q sentence-transformers\n",
        "\n",
        "# Datasets : Permet de charger des jeux de donn√©es depuis Hugging Face.\n",
        "!pip install -q datasets\n",
        "\n",
        "# Faiss-cpu : Librairie pour les recherches vectorielles rapides (m√™me sur CPU).\n",
        "!pip install -q faiss-cpu\n",
        "\n",
        "# langchain-community : Fournit les loaders et connecteurs suppl√©mentaires pour Langchain.\n",
        "!pip install -U langchain-community\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ √âtape 2 : Chargement du dataset"
      ],
      "metadata": {
        "id": "STqrpwpntyO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-1 : Installer encore une fois datasets pour √™tre s√ªr que tout fonctionne"
      ],
      "metadata": {
        "id": "fdFRLR66uH8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq datasets"
      ],
      "metadata": {
        "id": "n3r3yIsetzuE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2-2 2. Importer le loader depuis Langchain"
      ],
      "metadata": {
        "id": "rzsp4aAxuhxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import HuggingFaceDatasetLoader\n"
      ],
      "metadata": {
        "id": "BHK6eKM3uoG8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2-3 D√©finir les param√®tres du dataset\n"
      ],
      "metadata": {
        "id": "GetRHdHdu161"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nom du dataset depuis Hugging Face\n",
        "dataset_name = \"databricks/databricks-dolly-15k\"\n",
        "# Nom de la colonne contenant le texte que nous voulons utiliser\n",
        "page_content_column = \"context\"\n"
      ],
      "metadata": {
        "id": "T1lLiXsovBtM"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2-4. Cr√©er le loader et charger les documents"
      ],
      "metadata": {
        "id": "5AEEctz0vVKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cr√©ation du loader avec le nom du dataset et la colonne cible\n",
        "loader = HuggingFaceDatasetLoader(dataset_name, page_content_column)\n",
        "\n",
        "# Chargement effectif des documents\n",
        "data = loader.load()\n",
        "\n",
        "# (Optionnel) On affiche les deux premiers documents pour v√©rifier\n",
        "print(data[:2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SOr4Nn5vdUU",
        "outputId": "9900610d-1f49-4613-8fd8-dc88ff3e1059"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'instruction': 'When did Virgin Australia start operating?', 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.', 'category': 'closed_qa'}, page_content='\"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia\\'s domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\"'), Document(metadata={'instruction': 'Which is a species of fish? Tope or Rope', 'response': 'Tope', 'category': 'classification'}, page_content='\"\"')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## √âtape 3 : Splitter les documents (d√©coupage en morceaux)"
      ],
      "metadata": {
        "id": "6dVWuySNwO38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# On importe l'outil de d√©coupage r√©cursif de texte\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Cr√©ation d'une instance de TextSplitter\n",
        "# - chunk_size : taille de chaque morceau (1000 caract√®res ici)\n",
        "# - chunk_overlap : nombre de caract√®res communs entre deux morceaux successifs (150 ici)\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=150\n",
        ")\n",
        "\n",
        "# D√©coupage des documents pr√©c√©demment charg√©s\n",
        "docs = text_splitter.split_documents(data)\n",
        "\n",
        "#  Affichage du premier document pour v√©rifier le r√©sultat\n",
        "print(docs[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2rCt981wQr0",
        "outputId": "6b49f8d6-4d43-42ca-da63-606052685852"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='\"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\"' metadata={'instruction': 'When did Virgin Australia start operating?', 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.', 'category': 'closed_qa'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##√âtape 4 : Cr√©er les embeddings"
      ],
      "metadata": {
        "id": "GcV2oPw6w49U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# On importe la classe pour cr√©er les embeddings depuis sentence-transformers\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# On d√©finit le mod√®le d'embedding √† utiliser (l√©ger et rapide pour CPU)\n",
        "modelPath = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "# Options du mod√®le : ici, on reste sur CPU\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "\n",
        "# Options d'encodage : pas de normalisation, ce qui peut √™tre utile selon le moteur de recherche\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "\n",
        "# Cr√©ation de l'objet embeddings\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=modelPath,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "#  teste avec une phrase simple\n",
        "text = \"This is a test document.\"\n",
        "query_result = embeddings.embed_query(text)\n",
        "\n",
        "# affiche les 3 premi√®res valeurs du vecteur pour v√©rifier\n",
        "print(query_result[:3])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Zckjq5ow8aF",
        "outputId": "e6ae5d77-2e37-459a-d5cd-01858e8ac700"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.03833857178688049, 0.1234646737575531, -0.028642933815717697]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## R√©sultat de l'embedding :\n",
        "j'ai un vecteur de sortie :\n",
        "\n",
        "[-0.038338541984558105, 0.12346471846103668, -0.02864297851920128]\n",
        " Cela signifie que la phrase \"This is a test document.\" a √©t√© convertie en une repr√©sentation num√©rique s√©mantique, utilisable pour la recherche."
      ],
      "metadata": {
        "id": "kbtg-JXYxlAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## √âtape 5 : Cr√©er le vector store FAISS"
      ],
      "metadata": {
        "id": "w1bnhwe9yJbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzyFVOEezqcd",
        "outputId": "4ad04080-100c-4a20-a3b7-58021ce5a076"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.68)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.5)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Cr√©ation du vector store √† partir des documents d√©coup√©s et des embeddings\n",
        "db = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "# Cr√©ation du retriever √† partir de la base FAISS\n",
        "retriever = db.as_retriever()\n",
        "\n",
        "# Test : recherche d'un document similaire √† une question\n",
        "similar_docs = retriever.get_relevant_documents(\"When did Virgin Australia start operating?\")\n",
        "\n",
        "# Affichage du premier document trouv√©\n",
        "print(similar_docs[0])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmddFxuHyMxE",
        "outputId": "9fcc7b1c-835f-4866-ea65-0d36a3f282a8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='\"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\"' metadata={'instruction': 'When did Virgin Australia start operating?', 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.', 'category': 'closed_qa'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## R√©sultat:\n",
        "syst√®me est maintenant capable de retrouver le contenu le plus pertinent √† partir d‚Äôune question gr√¢ce aux embeddings s√©mantiques."
      ],
      "metadata": {
        "id": "oGtRdrrk3ZPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## √âtape 6 : Chargement du mod√®le de r√©ponse (LLM)"
      ],
      "metadata": {
        "id": "9gy6Aq2r3nb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6-1. Charger le mod√®le et le tokenizer :"
      ],
      "metadata": {
        "id": "tYvY262I3t0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "# Nom du mod√®le (petit, rapide, optimis√© pour CPU)\n",
        "model_name = \"Intel/dynamic_tinybert\"\n",
        "\n",
        "# Chargement du tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Chargement du mod√®le\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpzDG-Y335zN",
        "outputId": "dbaa1405-5e0c-434d-f64e-dad4741d1ff1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Invalid model-index. Not loading eval results into CardData.\n",
            "WARNING:huggingface_hub.repocard_data:Invalid model-index. Not loading eval results into CardData.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6-2. Cr√©er le pipeline Hugging Face"
      ],
      "metadata": {
        "id": "Y46G2j_27ClQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cr√©ation d'un pipeline de question-r√©ponse\n",
        "qa_pipeline = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_tensors='pt'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtaWTgHD7LhX",
        "outputId": "3f665cb1-270a-4767-a638-8cb8b0c0e772"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6-3 3. Int√©grer le pipeline dans LangChain"
      ],
      "metadata": {
        "id": "i6hIWQKo7UHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline(\n",
        "    pipeline=qa_pipeline,\n",
        "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512}\n",
        ")\n"
      ],
      "metadata": {
        "id": "NZHUX5zg7cmO"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## √âtape 7 : Construire la cha√Æne de question-r√©ponse (RetrievalQA)"
      ],
      "metadata": {
        "id": "cPYNsqMl7-MP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Cr√©ation du retriever √† partir de la base FAISS\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 4})  # k = nombre de documents √† r√©cup√©rer\n",
        "\n",
        "# Construction de la cha√Æne RetrievalQA\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,                      # Le mod√®le de r√©ponse\n",
        "    chain_type=\"refine\",         # M√©thode de r√©ponse (refine = am√©liorations progressives)\n",
        "    retriever=retriever,         # Le syst√®me de recherche (FAISS)\n",
        "    return_source_documents=False  # On ne retourne que la r√©ponse finale\n",
        ")\n"
      ],
      "metadata": {
        "id": "f4zNpUIa8McH"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## √âtape 8 : Tester le syst√®me RAG\n",
        "\n",
        "\n",
        "L'objectif est de poser une question en langage naturel, que le syst√®me va :\n",
        "\n",
        "* √âl√©ment de liste\n",
        "\n",
        "* interpr√©ter,\n",
        "\n",
        "* utiliser pour retrouver les documents pertinents via FAISS,\n",
        "\n",
        "* transmettre au LLM (Intel/dynamic_tinybert) pour g√©n√©rer une r√©ponse."
      ],
      "metadata": {
        "id": "kbyOoEHZ8ljo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. R√©cup√©rer les documents pertinents via FAISS\n",
        "retrieved_docs = retriever.invoke(\"What is cheesemaking?\")\n",
        "\n",
        "# 2. Concat√©ner leur contenu en un seul contexte\n",
        "context = \" \".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "# 3. Appeler le pipeline de question-r√©ponse directement avec question + contexte\n",
        "response = qa_pipeline(\n",
        "    question=\"What is cheesemaking?\",\n",
        "    context=context\n",
        ")\n",
        "\n",
        "# 4. Afficher uniquement la r√©ponse\n",
        "print(\"üß† R√©ponse g√©n√©r√©e :\", response[\"answer\"])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X898ZFXR8onv",
        "outputId": "7564adcf-9eef-41c4-8b33-fa3ebc119703"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† R√©ponse g√©n√©r√©e : to control the spoiling of milk into cheese\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion : Je viens de construire Tu viens de construire un RAG complet"
      ],
      "metadata": {
        "id": "nWYrUspdCBm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##les √©tapes suivies :\n",
        "\n",
        "\n",
        "| √âtape | Description                                                        |\n",
        "| ----- | ------------------------------------------------------------------ |\n",
        "| ‚úÖ 1   | Installation de tous les outils n√©cessaires                        |\n",
        "| ‚úÖ 2   | Chargement d‚Äôun dataset r√©el depuis Hugging Face                   |\n",
        "| ‚úÖ 3   | D√©coupage intelligent des documents                                |\n",
        "| ‚úÖ 4   | G√©n√©ration des embeddings via `sentence-transformers`              |\n",
        "| ‚úÖ 5   | Indexation vectorielle avec FAISS                                  |\n",
        "| ‚úÖ 6   | Chargement d‚Äôun LLM de question-r√©ponse                            |\n",
        "| ‚úÖ 7   | Recherche s√©mantique dans les documents                            |\n",
        "| ‚úÖ 8   | G√©n√©ration de r√©ponses naturelles √† partir des documents r√©cup√©r√©s |\n"
      ],
      "metadata": {
        "id": "9cI8l5QRCWRI"
      }
    }
  ]
}