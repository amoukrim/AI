{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNtiTrJ4gmu040SqbRhaKZ5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e54030ee06194660afa16290e9de4a10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e5b34e692a9d403e80fdca3981a2379c",
              "IPY_MODEL_a2e2647844d94a58a56ff67b98dbce7b",
              "IPY_MODEL_2767e2bd8b034382866ab07a701315b2"
            ],
            "layout": "IPY_MODEL_8f0d18c3bc2b4b13999cd16b98128b0d"
          }
        },
        "e5b34e692a9d403e80fdca3981a2379c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e418f8beb9654e74ae724f27331db57b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_805817e127ea4b2aa144e0bfb27d64c3",
            "value": "Map:‚Äá100%"
          }
        },
        "a2e2647844d94a58a56ff67b98dbce7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf9cfdb285b64ae99ccabb01ce213be9",
            "max": 4000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2fc0a3a1494c48858c2cc59ed7135e49",
            "value": 4000
          }
        },
        "2767e2bd8b034382866ab07a701315b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_981f1bf39fc74df6bdf9b69198848214",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e64aeef743244591b066a5e6b5232713",
            "value": "‚Äá4000/4000‚Äá[00:01&lt;00:00,‚Äá3708.60‚Äáexamples/s]"
          }
        },
        "8f0d18c3bc2b4b13999cd16b98128b0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e418f8beb9654e74ae724f27331db57b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "805817e127ea4b2aa144e0bfb27d64c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf9cfdb285b64ae99ccabb01ce213be9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fc0a3a1494c48858c2cc59ed7135e49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "981f1bf39fc74df6bdf9b69198848214": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e64aeef743244591b066a5e6b5232713": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "386da149ca8049e6a871cf68ddeeb4ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9527193f827f4929a7619ec73f759487",
              "IPY_MODEL_3ce63047cc004e3bb35ebc9816102b23",
              "IPY_MODEL_30585fafb71d4cb69c3195ef1310bdee"
            ],
            "layout": "IPY_MODEL_317e1d9633d440cca2a49d068323890c"
          }
        },
        "9527193f827f4929a7619ec73f759487": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a481589d4484153a1b9710266b0aea4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3c97ebc959c44657b730b4ec6219a084",
            "value": "Map:‚Äá100%"
          }
        },
        "3ce63047cc004e3bb35ebc9816102b23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b1a7acb1b8846b583cd484766a0e386",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3026b91cc3cd49849327cae6223cd5e1",
            "value": 1000
          }
        },
        "30585fafb71d4cb69c3195ef1310bdee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c970637b8263426f93aa0efb631be5e8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_57f0ae01209542ab909ad7c3b6e4b055",
            "value": "‚Äá1000/1000‚Äá[00:00&lt;00:00,‚Äá3220.10‚Äáexamples/s]"
          }
        },
        "317e1d9633d440cca2a49d068323890c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a481589d4484153a1b9710266b0aea4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c97ebc959c44657b730b4ec6219a084": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b1a7acb1b8846b583cd484766a0e386": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3026b91cc3cd49849327cae6223cd5e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c970637b8263426f93aa0efb631be5e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57f0ae01209542ab909ad7c3b6e4b055": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amoukrim/AI/blob/main/Week6/DailyChallenge/dailyChallengew_6_d3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# @Author Adil MOUKRIM\n",
        "@ Daily Challenge: Fine-Tuning GPT-2 for SMS Spam Classification (Legacy transformers API)\n",
        "\n",
        "\n",
        "In this daily challenge, you‚Äôll fine-tune a pre-trained GPT-2 model to classify SMS messages as spam or ham (not spam). We‚Äôll work through loading the dataset, inspecting its schema, tokenizing examples, adapting to an older transformers version, and running training and evaluation with the classic do_train/do_eval flags.\n",
        "\n",
        "\n",
        "\n",
        "üë©‚Äçüè´ üë©üèø‚Äçüè´ What You‚Äôll learn\n",
        "How to load and explore a custom text-classification dataset\n",
        "Inspecting and aligning column names for tokenization\n",
        "Tokenizing text for GPT-2 (with its peculiar padding setup)\n",
        "Initializing GPT2ForSequenceClassification\n",
        "Defining and computing multiple evaluation metrics\n",
        "Configuring TrainingArguments for transformers < 4.4 (using do_train, eval_steps, etc.)\n",
        "Running fine-tuning with Trainer and interpreting results\n",
        "Common pitfalls when using legacy APIs\n",
        "\n",
        "\n",
        "üõ†Ô∏è What you will create\n",
        "By the end of this challenge, you will have built:\n",
        "\n",
        "A tokenized SMS dataset compatible with GPT-2‚Äôs requirements, including custom padding and truncation.\n",
        "A fine-tuned GPT2ForSequenceClassification model that can accurately label incoming SMS messages as spam or ham.\n",
        "A complete training pipeline using the legacy do_train/do_eval flags in TrainingArguments, with periodic checkpointing, logging, and evaluation.\n",
        "A set of evaluation metrics (accuracy, precision, recall, F1) computed at each validation step and summarized after training.\n",
        "A reusable Jupyter notebook that ties everything together‚Äîfrom dataset loading and inspection, through model initialization and tokenization, to training, evaluation, and results interpretation.\n",
        "\n",
        "\n",
        "üíº Prerequisites\n",
        "Python 3.7+\n",
        "Installed packages: datasets, evaluate, transformers>=4.0.0,<4.4.0\n",
        "Basic familiarity with Hugging Face‚Äôs datasets and transformers libraries\n",
        "GitHub or Colab access for executing the notebook\n",
        "A Hugging Face API and a WeightAndBiases API, for instructions on how to get it, click here.\n",
        "\n",
        "\n",
        "Task\n",
        "We will guide you through making a fine-tuning a GPT-2 model to classify SMS messages as spam or ham using an older version of transformers (<4.4). Follow the steps below and complete the ‚ÄúTODO‚Äù in the code.\n",
        "\n",
        "1. Setup : Install required packages datasets, evaluate and transformers[sentencepiece].\n",
        "\n",
        "%pip install --quiet datasets evaluate transformers[sentencepiece]\n",
        "\n",
        "\n",
        "2. Load & Inspect Dataset :\n",
        "\n",
        "from datasets import TODO #import load_dataset\n",
        "TODO # import pandas\n",
        "\n",
        "# Load the UCI SMS Spam dataset (sms_spam) from Hugging Face hub\n",
        "raw = TODO\n",
        "\n",
        "# We'll use 4,000 for train, 1,000 for validation\n",
        "train_ds = TODO\n",
        "val_ds   = TODO\n",
        "\n",
        "TODO  # print the features of the train dataset. It should show 'sms' and 'label'\n",
        "\n",
        "\n",
        "3. Tokenization :\n",
        "\n",
        "from transformers import TODO # import GPT2Tokenizer\n",
        "\n",
        "\n",
        "model_name = TODO #load the tokenize, we will use GPT2\n",
        "tokenizer  = TODO\n",
        "# GPT-2 has no pad token by default‚Äîset it to eos\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_fn(examples):\n",
        "    # returns input_ids, attention_mask; keep max_length small for SMS\n",
        "    return tokenizer(\n",
        "        examples[\"sms\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=64\n",
        "    )\n",
        "\n",
        "train_tok = TODO #apply the tokenization by loading the subset using .map function\n",
        "val_tok   = TODO #apply the tokenization by loading the subset using .map function\n",
        "\n",
        "\n",
        "\n",
        "4. Model Initialization\n",
        "\n",
        "import torch\n",
        "TODO  #import GPT2ForSequenceClassification\n",
        "\n",
        "model = GPT2ForSequenceClassification.from_pretrained( # Load GPT-2 with sequence classification head\n",
        "    model_name,\n",
        "    num_labels=TODO,           # spam vs. ham\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "\n",
        "5. Metrics Definition\n",
        "\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "accuracy  = evaluate.load(\"accuracy\")\n",
        "precision = # apply the function used for accurracy but for precision\n",
        "recall    = # apply the function used for accurracy but for recall\n",
        "f1        = # apply the function used for accurracy but for F1\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    logits, labels = pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\":  accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
        "        \"precision\": TODO, # apply the function used for accurracy but for precision\n",
        "        \"recall\":    TODO, # apply the function used for accurracy but for recall\n",
        "        \"f1\":        TODO # apply the function used for accurracy but for F1\n",
        "    }\n",
        "\n",
        "\n",
        "In an imbalanced dataset like SMS spam (often more ‚Äúham‚Äù than ‚Äúspam‚Äù), why is it important to track precision and recall alongside accuracy?\n",
        "How would you interpret a model that achieves high accuracy but low recall on the spam class?\n",
        "\n",
        "\n",
        "6. TrainingArguments Configuration\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=TODO\n",
        "    do_train=True,                 # turn on training\n",
        "    do_eval=True,                  # turn on evaluation\n",
        "    eval_steps=TODO,                # run .evaluate() every 500 steps\n",
        "    save_steps=TODO,                # save a checkpoint every 500 steps\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=TODO,             # log metrics every 500 steps\n",
        "\n",
        "    per_device_train_batch_size=TODO,\n",
        "    per_device_eval_batch_size=TODO,\n",
        "    num_train_epochs=TODO,\n",
        "    learning_rate=TODO,\n",
        "    weight_decay=TODO,\n",
        "\n",
        "    report_to=None,                # disable integrations\n",
        "    save_total_limit=1,            # only keep last checkpoint\n",
        ")\n",
        "\n",
        "\n",
        "What effect does weight_decay have during fine-tuning? When might you choose a higher or lower value?\n",
        "\n",
        "\n",
        "7. Train & Evaluate\n",
        "\n",
        "# Train\n",
        "from transformers import Trainer\n",
        "# you need to have your wandb api key ready to paste in the command line\n",
        "trainer = Trainer(\n",
        "    model=TODO,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=val_tok,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "#Evaluate\n",
        "metrics = TODO\n",
        "print(metrics)\n",
        "# Expect something like: {\"eval_loss\": ..., \"eval_accuracy\": 0.98, ...}\n",
        "\n",
        "\n",
        "\n",
        "Interpret your results.\n"
      ],
      "metadata": {
        "id": "r-cQdG9tTqXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "√âtape 1 ‚Äì Configuration de l‚Äôenvironnement"
      ],
      "metadata": {
        "id": "aYGsXXFvUbKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzP-2KmVZlQE",
        "outputId": "cdb86fe5-4c06-4352-c920-f8937bb17b03"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.9)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "p-shoGD_Touy"
      },
      "outputs": [],
      "source": [
        "# Installation des d√©pendances\n",
        "!pip install --quiet evaluate transformers[sentencepiece]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# √âtape 2 ‚Äì Chargement et exploration du dataset SMS Spam\n",
        "\n"
      ],
      "metadata": {
        "id": "Ks9gz2JnUyuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Chargement du jeu de donn√©es \"sms_spam\" depuis HF Hub\n",
        "raw = load_dataset(\"ucirvine/sms_spam\")\n",
        "\n",
        "# S√©paration du jeu (4000 entra√Ænement, 1000 validation)\n",
        "train_ds = raw[\"train\"].shuffle(seed=42).select(range(4000))\n",
        "val_ds = raw[\"train\"].shuffle(seed=42).select(range(4000, 5000))\n",
        "\n",
        "# Affichage du sch√©ma\n",
        "print(train_ds.features)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtrO4gMAUxdT",
        "outputId": "8fde5217-8f19-4e14-d31d-c027d20ed32a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sms': Value('string'), 'label': ClassLabel(names=['ham', 'spam'])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement du dataset r√©ussie :\n",
        "üì® \"sms\" est la colonne contenant les messages.\n",
        "\n",
        "üè∑Ô∏è \"label\" est la colonne contenant les classes ham (0) et spam (1).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IReIk6hqbzFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# √âtape 3 ‚Äî Tokenization pour GPT-2"
      ],
      "metadata": {
        "id": "kvSrTArDdCsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Nom du mod√®le pr√©-entra√Æn√© (de base)\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "# Chargement du tokenizer GPT-2\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# üí° GPT-2 ne poss√®de pas de token de padding ‚Üí on utilise le token de fin\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Fonction de tokenization : transforme chaque message en input_ids + attention_mask\n",
        "def tokenize_fn(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"sms\"],            # le texte du SMS\n",
        "        padding=\"max_length\",       # on remplit √† la m√™me taille (obligatoire pour batching)\n",
        "        truncation=True,            # on coupe s'il d√©passe\n",
        "        max_length=64               # taille max courte car ce sont des SMS\n",
        "    )\n",
        "\n",
        "# Application de la fonction sur chaque message du jeu d'entra√Ænement et de validation\n",
        "train_tok = train_ds.map(tokenize_fn, batched=True)\n",
        "val_tok   = val_ds.map(tokenize_fn, batched=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "e54030ee06194660afa16290e9de4a10",
            "e5b34e692a9d403e80fdca3981a2379c",
            "a2e2647844d94a58a56ff67b98dbce7b",
            "2767e2bd8b034382866ab07a701315b2",
            "8f0d18c3bc2b4b13999cd16b98128b0d",
            "e418f8beb9654e74ae724f27331db57b",
            "805817e127ea4b2aa144e0bfb27d64c3",
            "bf9cfdb285b64ae99ccabb01ce213be9",
            "2fc0a3a1494c48858c2cc59ed7135e49",
            "981f1bf39fc74df6bdf9b69198848214",
            "e64aeef743244591b066a5e6b5232713",
            "386da149ca8049e6a871cf68ddeeb4ca",
            "9527193f827f4929a7619ec73f759487",
            "3ce63047cc004e3bb35ebc9816102b23",
            "30585fafb71d4cb69c3195ef1310bdee",
            "317e1d9633d440cca2a49d068323890c",
            "1a481589d4484153a1b9710266b0aea4",
            "3c97ebc959c44657b730b4ec6219a084",
            "6b1a7acb1b8846b583cd484766a0e386",
            "3026b91cc3cd49849327cae6223cd5e1",
            "c970637b8263426f93aa0efb631be5e8",
            "57f0ae01209542ab909ad7c3b6e4b055"
          ]
        },
        "id": "8wn0uOjPdxY8",
        "outputId": "1e76af80-2253-43eb-968b-05770bf0e7d8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e54030ee06194660afa16290e9de4a10"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "386da149ca8049e6a871cf68ddeeb4ca"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## R√©sultat de la Tokenization :\n",
        "\n",
        "| √âl√©ment                    | Description                                                               |\n",
        "| -------------------------- | ------------------------------------------------------------------------- |\n",
        "| üì¶ `tokenizer`             | C‚Äôest le **tokenizer GPT-2**, pr√©entra√Æn√© avec un vocabulaire sp√©cifique. |\n",
        "| ‚úÖ Padding                  | J'ai utilis√© `eos_token` comme `pad_token` (GPT-2 n‚Äôa pas de PAD natif). |\n",
        "| ‚úÇÔ∏è Truncation              | Limit√© chaque SMS √† 64 tokens max (largement suffisant pour des SMS).     |\n",
        "| üóÇÔ∏è Tokenisation appliqu√©e | Aux **4000 SMS d‚Äôentra√Ænement** et aux **1000 SMS de validation**.        |\n"
      ],
      "metadata": {
        "id": "Y0s_9VaMeiyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher quelques exemples tokenis√©s\n",
        "train_tok[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "O6C-fuvQeNvc",
        "outputId": "ad971945-5f85-4780-c583-bd8c938016de"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sms': 'sports fans - get the latest sports news str* 2 ur mobile 1 wk FREE PLUS a FREE TONE Txt SPORT ON to 8007 www.getzed.co.uk 0870141701216+ norm 4txt/120p \\n',\n",
              " 'label': 1,\n",
              " 'input_ids': [32945,\n",
              "  3296,\n",
              "  532,\n",
              "  651,\n",
              "  262,\n",
              "  3452,\n",
              "  5701,\n",
              "  1705,\n",
              "  965,\n",
              "  9,\n",
              "  362,\n",
              "  2956,\n",
              "  5175,\n",
              "  352,\n",
              "  266,\n",
              "  74,\n",
              "  17189,\n",
              "  48635,\n",
              "  257,\n",
              "  17189,\n",
              "  309,\n",
              "  11651,\n",
              "  309,\n",
              "  742,\n",
              "  6226,\n",
              "  9863,\n",
              "  6177,\n",
              "  284,\n",
              "  10460,\n",
              "  22,\n",
              "  7324,\n",
              "  13,\n",
              "  1136,\n",
              "  8863,\n",
              "  13,\n",
              "  1073,\n",
              "  13,\n",
              "  2724,\n",
              "  657,\n",
              "  5774,\n",
              "  28645,\n",
              "  1558,\n",
              "  486,\n",
              "  20666,\n",
              "  10,\n",
              "  2593,\n",
              "  604,\n",
              "  14116,\n",
              "  14,\n",
              "  10232,\n",
              "  79,\n",
              "  220,\n",
              "  198,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256],\n",
              " 'attention_mask': [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0]}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# √âtape 4 ‚Äì Initialiser le mod√®le GPT-2 avec une t√™te de classification\n",
        "l'Objectif est d'initialiser GPT-2 avec une t√™te de classification binaire (spam ou ham). GPT-2 est √† l‚Äôorigine un mod√®le de g√©n√©ration de texte, donc je  dois l‚Äôadapter pour une t√¢che de classification."
      ],
      "metadata": {
        "id": "5nVTI0lHiYRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üì¶ Import du mod√®le de classification bas√© sur GPT-2\n",
        "from transformers import GPT2ForSequenceClassification\n",
        "model_name = \"gpt2\"\n",
        "# üéØ On initialise GPT-2 avec une \"classification head\"\n",
        "model = GPT2ForSequenceClassification.from_pretrained(\n",
        "    model_name,             # 'gpt2' ou autre si tu veux un mod√®le plus gros\n",
        "    num_labels=2,           # 0 = ham, 1 = spam ‚Üí classification binaire\n",
        "    pad_token_id=tokenizer.eos_token_id  # n√©cessaire pour √©viter des erreurs avec le padding\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCPJ7Wq6iuEc",
        "outputId": "41af1997-05fa-424e-b848-fd3e1f99a5a5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuR6HbLmjWH1",
        "outputId": "582a4fe7-ec73-4878-efb2-a4f1eadfa156"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2ForSequenceClassification(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D(nf=2304, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=768)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=3072, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=3072)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úÖ √âtape 5 : D√©finir les m√©triques d‚Äô√©valuation.\n"
      ],
      "metadata": {
        "id": "qr83eXBRmU1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate        # biblioth√®que Hugging Face pour les m√©triques standards\n",
        "import numpy as np     # n√©cessaire pour le traitement des logits\n",
        "\n",
        "# üìà Chargement des m√©triques depuis evaluate\n",
        "accuracy  = evaluate.load(\"accuracy\")\n",
        "precision = evaluate.load(\"precision\")\n",
        "recall    = evaluate.load(\"recall\")\n",
        "f1        = evaluate.load(\"f1\")\n",
        "\n",
        "# üßÆ Fonction de calcul des m√©triques, appel√©e automatiquement √† chaque √©valuation\n",
        "def compute_metrics(pred):\n",
        "    logits, labels = pred                     # pr√©dictions brutes du mod√®le (logits) et vraies √©tiquettes\n",
        "    preds = np.argmax(logits, axis=-1)        # on convertit les logits en classes pr√©dites (0 ou 1)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\":  accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
        "        \"precision\": precision.compute(predictions=preds, references=labels)[\"precision\"],\n",
        "        \"recall\":    recall.compute(predictions=preds, references=labels)[\"recall\"],\n",
        "        \"f1\":        f1.compute(predictions=preds, references=labels)[\"f1\"]\n",
        "    }"
      ],
      "metadata": {
        "id": "8nsy_etYmaOF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##√âtape 6 ‚Äì TrainingArguments\n",
        "\n",
        "L'objectif est de configurerles hyperparam√®tres de l‚Äôentra√Ænement, comme le batch size, le taux d‚Äôapprentissage, la fr√©quence d‚Äô√©valuation, etc."
      ],
      "metadata": {
        "id": "ntNozN-mmu9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",        # üìÇ O√π sauvegarder les checkpoints du mod√®le\n",
        "\n",
        "    do_train=True,                 # ‚úÖ Active l'entra√Ænement\n",
        "    do_eval=True,                  # ‚úÖ Active l'√©valuation pendant l'entra√Ænement\n",
        "\n",
        "    eval_steps=500,                # üß™ √âvaluer toutes les 500 √©tapes\n",
        "    save_steps=500,                # üíæ Sauvegarder un checkpoint toutes les 500 √©tapes\n",
        "    logging_steps=500,             # ü™µ Journaliser les m√©triques toutes les 500 √©tapes\n",
        "\n",
        "    per_device_train_batch_size=8,     # üì¶ Nombre d'exemples par lot en entra√Ænement\n",
        "    per_device_eval_batch_size=8,      # üì¶ Idem pour validation\n",
        "    num_train_epochs=3,                # üîÅ Nombre de fois qu'on passe sur le jeu d'entra√Ænement\n",
        "\n",
        "    learning_rate=5e-5,            # ‚öôÔ∏è Taux d'apprentissage\n",
        "    weight_decay=0.01,             # üßΩ R√©gularisation L2 pour √©viter le surapprentissage\n",
        "\n",
        "    report_to=[],                # üö´ D√©sactiver WandB, TensorBoard, etc. (√† activer si besoin)\n",
        "    save_total_limit=1             # üîÅ Ne garder que le dernier checkpoint\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "vuqmBsllmz6F"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## √âtape 7 : Entra√Æner et √©valuer le mod√®le avec Trainer"
      ],
      "metadata": {
        "id": "yP38AhiTnlKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "# üß† Cr√©e le Trainer avec tous les √©l√©ments n√©cessaires\n",
        "trainer = Trainer(\n",
        "    model=model,                        # üéØ le mod√®le GPT-2 √† entra√Æner\n",
        "    args=training_args,                # ‚öôÔ∏è les hyperparam√®tres d√©finis pr√©c√©demment\n",
        "    train_dataset=train_tok,           # üìä dataset d'entra√Ænement tokenis√©\n",
        "    eval_dataset=val_tok,              # üìä dataset de validation tokenis√©\n",
        "    compute_metrics=compute_metrics    # üìè fonction pour √©valuer le mod√®le\n",
        ")\n",
        "\n",
        "# üöÄ Lancement de l'entra√Ænement\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Fh5Fy-wenzmG",
        "outputId": "44049ce4-d4c0-4473-cb1a-1abaa19613bc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 05:04, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.118700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.039000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.009500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1500, training_loss=0.0557409995396932, metrics={'train_runtime': 305.9798, 'train_samples_per_second': 39.218, 'train_steps_per_second': 4.902, 'total_flos': 391945125888000.0, 'train_loss': 0.0557409995396932, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "√âvaluer sur le set de validation"
      ],
      "metadata": {
        "id": "lr7sjC8Tn5W1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîç √âvaluation finale sur le dataset de validation\n",
        "metrics = trainer.evaluate()\n",
        "\n",
        "# üì¢ Affichage des r√©sultats\n",
        "print(metrics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "AQaNGNwKn9Gl",
        "outputId": "e23ed12a-cdda-4279-a227-5821148c1a3b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.037382956594228745, 'eval_accuracy': 0.995, 'eval_precision': 0.9831932773109243, 'eval_recall': 0.975, 'eval_f1': 0.9790794979079498, 'eval_runtime': 4.0378, 'eval_samples_per_second': 247.658, 'eval_steps_per_second': 30.957, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Interpr√©tation m√©trique par m√©trique :\n",
        "\n",
        "| M√©trique      | R√©sultat | Interpr√©tation                                                                 |\n",
        "| ------------- | -------- | ------------------------------------------------------------------------------ |\n",
        "| **Loss**      | `0.037`  | Tr√®s faible, le mod√®le a bien appris √† distinguer les classes                  |\n",
        "| **Accuracy**  | `99.5%`  | Tr√®s haut, montre que le mod√®le est globalement tr√®s fiable                    |\n",
        "| **Precision** | `98.3%`  | Peu de faux positifs ‚Üí tr√®s peu de messages normaux d√©tect√©s √† tort comme spam |\n",
        "| **Recall**    | `97.5%`  | Peu de spams sont rat√©s (faux n√©gatifs faibles)                                |\n",
        "| **F1-score**  | `97.9%`  | Excellent √©quilibre entre pr√©cision et rappel                                  |\n",
        "\n",
        "\n",
        "--> GPT-2 est entra√Æn√© avec succ√®s pour la classification SMS et les r√©sultats sont excellents.\n"
      ],
      "metadata": {
        "id": "IPI0vpLQs2RW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖ Conclusion\n",
        "Le mod√®le GPT-2 est fine-tun√© :\n",
        "\n",
        "* Pr√©dit les SMS spam avec tr√®s haute pr√©cision\n",
        "\n",
        "* Rate tr√®s peu de spams\n",
        "\n",
        "* G√©n√©ralise bien sans surapprentissage apparent (vu le loss bas et l‚Äô√©quilibre des scores)\n",
        "\n"
      ],
      "metadata": {
        "id": "jceFlQ3TtdHV"
      }
    }
  ]
}